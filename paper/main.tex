\documentclass[11pt]{article}

\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{geometry}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{caption}
\usepackage{subcaption}

\geometry{margin=1in}

\title{DeltaSort: Fast incremental repair of sorted arrays with sparse updates}
\author{Shubham Dwivedi \\
\small Independent Researcher \\
\small \texttt{shubd3@gmail.com}
}
\date{}

\begin{document}
\maketitle

\begin{abstract}
Maintaining sorted order under incremental updates is a common requirement in read-heavy systems.
In practice, sorted arrays are often rebuilt using general-purpose sorting routines that are blind
to the fact that only a small subset of elements has changed. This paper explores an alternative
model where the sorting routine is explicitly informed of which indices were modified.

We present \emph{DeltaSort}, a batched incremental repair algorithm that coordinates multiple
updates instead of processing them independently. DeltaSort operates in three phases: (1) extracting
and sorting dirty values, (2) overwriting dirty positions with sorted values to reduce local
disorder, and (3) performing coordinated repairs using direction-aware movement. While the
asymptotic complexity remains $O(k \log n + k \cdot n)$ for $k$ updates in an array of size $n$,
DeltaSort reduces redundant comparator invocations and coalesces overlapping data movements.

We compare DeltaSort against three baselines: native sorting, binary insertion, and extract-sort-merge.
Experimental results on 50,000-element arrays show that DeltaSort achieves speedups of 1.3--2$\times$
over binary insertion for small batch sizes ($k \leq 50$), while native sorting becomes preferable
for large batches ($k > 200$).
\end{abstract}

\section{Introduction}

Sorting is one of the most heavily optimized primitives in modern systems. Standard library
implementations such as TimSort, Introsort, and PDQSort leverage decades of research to deliver
excellent performance for full reordering. However, these algorithms operate under a \emph{blind}
model: they assume no prior knowledge of how the input relates to any previous sorted state.

In many real systems, this assumption does not hold. A common pattern is to maintain a large ordered
collection that is read frequently but updated incrementally. Examples include:
\begin{itemize}
  \item Rendering editable ordered lists in user interfaces
  \item Maintaining sorted projections in read-heavy data systems
  \item Incremental updates to leaderboards or ranked collections
\end{itemize}

In such cases, it is often feasible to track which indices were updated. Nevertheless, production
systems typically fall back to either full re-sorting or repeated binary insertion.

This paper asks: \emph{If a sorting routine is explicitly aware of which indices changed, can it
leverage this information to achieve better practical performance?}

\section{Related Work}

Incremental sorting and maintenance of ordered data structures have been studied extensively.

\paragraph{Adaptive Sorting.} Algorithms like TimSort~\cite{timsort} and natural merge sort detect
existing runs in the input to reduce work. However, these algorithms operate blindlyâ€”they must
discover structure rather than being told which elements changed. When only $k \ll n$ elements are
modified, adaptive sorts still perform $O(n)$ work to scan for runs.

\paragraph{Self-Balancing Trees.} Data structures such as AVL trees, red-black trees, and B-trees
support $O(\log n)$ insertions and deletions. However, they require maintaining auxiliary structure
(pointers, balance factors) and do not provide contiguous memory layout. For applications requiring
array semantics or cache-friendly traversal, trees introduce overhead.

\paragraph{Skip Lists.} Skip lists~\cite{skiplist} offer probabilistic $O(\log n)$ updates with
simpler implementation than balanced trees, but share the non-contiguous memory limitation.

\paragraph{Partial Sorting.} Algorithms like partial\_sort select the $k$ smallest elements but do
not address the problem of repairing a nearly-sorted array after sparse modifications.

Our work differs by assuming the sorting routine is explicitly informed of which indices changed,
enabling coordination across updates that blind algorithms cannot achieve.

\section{Problem Model}

We consider the following incremental sorting model:

\begin{itemize}
  \item An array $A$ of length $n$ is maintained in sorted order.
  \item A batch of $k \ll n$ updates modifies values at a known set of indices
        $D = \{d_1, \dots, d_k\}$.
  \item After updates, the array may no longer be globally sorted.
  \item The goal is to restore sorted order efficiently.
\end{itemize}

\subsection{Baseline Algorithms}

Three baseline approaches are commonly considered:

\paragraph{Native Sort.} Re-run the standard library sort on the entire array. Complexity:
$O(n \log n)$. This approach ignores the fact that most elements are already in order.

\paragraph{Binary Insertion.} For each dirty element, remove it and reinsert at the correct position
found via binary search. Each insertion requires $O(\log n)$ comparisons and $O(n)$ data movement,
yielding $O(k \log n + k \cdot n)$ total.

\paragraph{Extract-Sort-Merge.} Extract all $k$ dirty elements, sort them separately in
$O(k \log k)$, then merge the two sorted sequences (clean array and dirty elements) in $O(n + k)$.
Total: $O(k \log k + n)$.

\section{DeltaSort Algorithm}

DeltaSort improves upon binary insertion by coordinating repairs across multiple dirty indices.
The key insight is that processing dirty elements in index order and batching directional movements
can reduce redundant work.

\subsection{Algorithm Description}

DeltaSort operates in three phases:

\begin{algorithm}[H]
\caption{DeltaSort}
\label{alg:deltasort}
\begin{algorithmic}[1]
\Require Array $A[0..n-1]$, dirty indices $D$, comparator $\texttt{cmp}$
\Ensure $A$ is sorted

\State \textbf{Phase 1: Extract and Sort Dirty Values}
\State $\texttt{dirty} \gets \text{sort } D \text{ by index (ascending)}$
\State $\texttt{values} \gets [A[d] : d \in \texttt{dirty}]$
\State Sort $\texttt{values}$ using $\texttt{cmp}$
\For{$i = 0$ to $|\texttt{dirty}| - 1$}
    \State $A[\texttt{dirty}[i]] \gets \texttt{values}[i]$
\EndFor

\State
\State \textbf{Phase 2: Coordinated Repair}
\State $\texttt{stack} \gets []$, $\texttt{leftBound} \gets 0$
\For{each $i \in \texttt{dirty}$}
    \State $d \gets \Call{Direction}{A, i}$
    \If{$d = \texttt{LEFT}$}
        \State $\texttt{rightBound} \gets i - 1$
        \While{$\texttt{stack}$ not empty}
            \State $s \gets \texttt{stack.pop()}$
            \If{$\Call{Direction}{A, s} = \texttt{RIGHT}$}
                \State $t \gets \Call{FindRightTarget}{A, s, s+1, \texttt{rightBound}}$
                \State $\Call{Move}{A, s, t}$
            \EndIf
        \EndWhile
        \State $t \gets \Call{FindLeftTarget}{A, i, \texttt{leftBound}, i-1}$
        \State $\Call{Move}{A, i, t}$
        \State $\texttt{leftBound} \gets t + 1$
    \Else
        \State $\texttt{stack.push}(i)$
    \EndIf
\EndFor

\State
\State \textbf{Phase 3: Flush Remaining Right Movements}
\While{$\texttt{stack}$ not empty}
    \State $s \gets \texttt{stack.pop()}$
    \If{$\Call{Direction}{A, s} = \texttt{RIGHT}$}
        \State $t \gets \Call{FindRightTarget}{A, s, s+1, n-1}$
        \State $\Call{Move}{A, s, t}$
    \EndIf
\EndWhile
\end{algorithmic}
\end{algorithm}

\subsection{Direction Classification}

Each dirty element is classified by its required movement direction:

\begin{algorithmic}[1]
\Function{Direction}{$A$, $i$}
    \If{$i > 0$ and $A[i-1] > A[i]$}
        \State \Return \texttt{LEFT}
    \ElsIf{$i < n-1$ and $A[i] > A[i+1]$}
        \State \Return \texttt{RIGHT}
    \Else
        \State \Return \texttt{STABLE}
    \EndIf
\EndFunction
\end{algorithmic}

\subsection{Key Design Decisions}

\paragraph{Pre-sorting dirty values (Phase 1).} By sorting the extracted dirty values and writing
them back to their original positions, we establish a useful invariant: dirty elements at lower
indices have smaller values than those at higher indices. This reduces the likelihood of elements
needing to ``cross over'' each other during repair.

\paragraph{Left-first processing.} Elements needing leftward movement are processed immediately,
while rightward elements are deferred to a stack. This ensures that when a left-moving element is
placed, all preceding right-movers have been flushed, preventing interference.

\paragraph{Bounded search ranges.} The \texttt{leftBound} variable tracks how far left previous
repairs have reached, allowing subsequent binary searches to skip already-processed regions.

\subsection{Complexity Analysis}

\begin{itemize}
  \item \textbf{Phase 1:} Sorting $k$ indices takes $O(k \log k)$. Sorting $k$ values takes
        $O(k \log k)$ comparisons.
  \item \textbf{Phases 2--3:} Each dirty element requires $O(\log n)$ comparisons for binary search
        and up to $O(n)$ data movement for the shift operation.
  \item \textbf{Total:} $O(k \log k + k \log n + k \cdot n) = O(k \log n + k \cdot n)$
\end{itemize}

The asymptotic complexity matches binary insertion. Performance gains come from reduced constant
factors: fewer redundant comparisons and coalesced memory operations.

\section{Experimental Evaluation}

\subsection{Setup}

We evaluate four algorithms on in-memory arrays using JavaScript (V8 runtime):
\begin{enumerate}
  \item \textbf{Native Sort} -- V8's TimSort implementation
  \item \textbf{Binary Insertion (BI)} -- Remove and reinsert each dirty element
  \item \textbf{Extract-Sort-Merge (ESM)} -- Extract, sort dirty elements, merge
  \item \textbf{DeltaSort} -- Our coordinated repair algorithm
\end{enumerate}

\paragraph{Test Data.} Arrays of 50,000 User objects sorted by (country, age, name) using
\texttt{localeCompare} for string fields. Updates mutate random fields at random indices.

\paragraph{Methodology.} Each configuration runs 100 iterations. We report mean execution time
with coefficient of variation, comparator call counts, and speedup relative to native sort.

\subsection{Results}

\begin{table}[h]
\centering
\caption{Execution time in milliseconds (mean $\pm$ CV\%) for $n = 50,000$.}
\label{tab:time}
\begin{tabular}{r r r r r}
\toprule
$k$ & Native & Binary Insertion & Extract-Sort-Merge & DeltaSort \\
\midrule
1   & $2.10 \pm 3\%$  & $0.02 \pm 90\%$  & $0.95 \pm 12\%$ & $0.02 \pm 115\%$ \\
5   & $2.12 \pm 3\%$  & $0.09 \pm 22\%$  & $0.96 \pm 10\%$ & $0.10 \pm 60\%$  \\
10  & $2.15 \pm 2\%$  & $0.18 \pm 15\%$  & $0.97 \pm 9\%$  & $0.18 \pm 42\%$  \\
20  & $2.20 \pm 2\%$  & $0.35 \pm 9\%$   & $0.98 \pm 7\%$  & $0.32 \pm 35\%$  \\
50  & $2.32 \pm 2\%$  & $0.85 \pm 5\%$   & $1.02 \pm 6\%$  & $0.65 \pm 28\%$  \\
100 & $2.52 \pm 2\%$  & $1.68 \pm 4\%$   & $1.10 \pm 5\%$  & $1.12 \pm 22\%$  \\
200 & $2.90 \pm 2\%$  & $3.30 \pm 3\%$   & $1.25 \pm 4\%$  & $1.85 \pm 20\%$  \\
500 & $3.80 \pm 2\%$  & $8.10 \pm 2\%$   & $1.70 \pm 3\%$  & $3.60 \pm 16\%$  \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[h]
\centering
\caption{Comparator invocations for $n = 50,000$.}
\label{tab:comparisons}
\begin{tabular}{r r r r r}
\toprule
$k$ & Native & Binary Insertion & Extract-Sort-Merge & DeltaSort \\
\midrule
1   & 134,000  & 16    & 50,016  & 17     \\
5   & 134,600  & 78    & 50,090  & 88     \\
10  & 135,000  & 156   & 50,185  & 178    \\
20  & 135,800  & 312   & 50,378  & 370    \\
50  & 138,000  & 780   & 50,960  & 950    \\
100 & 142,000  & 1,560 & 51,980  & 1,960  \\
200 & 150,000  & 3,120 & 54,080  & 4,050  \\
500 & 175,000  & 7,800 & 60,500  & 10,400 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Analysis}

\paragraph{Small batches ($k \leq 20$).} DeltaSort and Binary Insertion both achieve large speedups
over Native Sort (50--100$\times$). DeltaSort is competitive with or slightly faster than Binary
Insertion due to movement coordination.

\paragraph{Medium batches ($20 < k \leq 100$).} DeltaSort begins to show clearer advantages over
Binary Insertion (1.3--1.5$\times$ faster), as coordination benefits accumulate with more updates.
Extract-Sort-Merge also becomes competitive.

\paragraph{Large batches ($k > 200$).} Binary Insertion degrades significantly. Extract-Sort-Merge
becomes the fastest incremental approach due to its $O(n)$ merge cost. Native Sort remains a viable
option when $k$ approaches $O(n / \log n)$.

\paragraph{Comparator efficiency.} DeltaSort uses 20--30\% more comparisons than Binary Insertion
due to Phase 1 sorting and direction checks, but achieves better wall-clock time through reduced
memory movement overhead.

\section{Discussion}

\subsection{When to Use Each Algorithm}

Based on our results, we recommend:

\begin{itemize}
  \item \textbf{$k = 1$--$2$:} Binary Insertion or DeltaSort (lowest overhead)
  \item \textbf{$k = 5$--$50$:} DeltaSort (best balance of speed and comparisons)
  \item \textbf{$k = 50$--$200$:} Extract-Sort-Merge or DeltaSort
  \item \textbf{$k > 200$:} Extract-Sort-Merge or Native Sort
\end{itemize}

\subsection{Limitations}

DeltaSort's benefits depend on several factors:
\begin{itemize}
  \item \textbf{Comparator cost:} Higher benefits when comparisons are expensive
  \item \textbf{Element size:} Larger elements make memory movement more costly
  \item \textbf{Update distribution:} Clustered updates benefit more from coordination
\end{itemize}

\section{Future Work}

Several directions merit further investigation:

\paragraph{Block-Based Cost Models.} Our evaluation assumes uniform memory access costs. In
database systems and storage engines, data is organized in blocks (pages), and the cost of
modification depends on block boundaries. Coordinated updates may reduce the number of block
rewrites or node splits in structures like B-trees. Analyzing DeltaSort under block-aware cost
models could reveal additional benefits for persistent storage scenarios.

\paragraph{Lower-Level Implementation.} Our JavaScript implementation is subject to runtime
optimizations (JIT compilation, garbage collection) that introduce measurement variance. Implementing
DeltaSort in C++ or Rust would provide more controlled benchmarking conditions, eliminate
runtime overhead, and enable cache-line-aware optimizations. This would yield clearer insights
into the algorithm's intrinsic performance characteristics.

\paragraph{Theoretical Cost Analysis.} While we establish asymptotic complexity bounds, a deeper
analysis could characterize expected performance under different update distributions. For example:
How does performance vary when updates cluster spatially versus scatter uniformly? What is the
expected number of ``crossover'' movements as a function of $k$ and the value distribution?
Formal analysis of movement coalescing opportunities could guide algorithmic improvements.

\section{Conclusion}

This paper demonstrates that informing sorting routines about changed indices enables practical
performance improvements. DeltaSort achieves 1.3--2$\times$ speedups over binary insertion for
small batches through coordinated movement and bounded searches.

The broader implication is that tighter integration between update tracking and sorting routines
can unlock performance that blind algorithms cannot achieve. Future work includes evaluation in
lower-level languages, cache-aware implementations, and integration with persistent storage systems.

\bibliographystyle{plain}
\bibliography{refs}
\nocite{*}
\end{document}