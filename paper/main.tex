\documentclass[11pt]{article}

\usepackage{amsmath,amssymb}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{geometry}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{pgfplotstable}
\usepackage{placeins}
\usepackage{float}
\pgfplotsset{compat=1.18}

% Path to benchmark data (different for article vs SEA version)
\newcommand{\rootdir}{.}

% Load benchmark metadata from CSV
\pgfplotstableread[col sep=comma]{\rootdir/figures/rust/metadata.csv}\rustmetadata
\pgfplotstablegetelem{0}{value}\of\rustmetadata \let\rustbenchmarkdate\pgfplotsretval
\pgfplotstablegetelem{1}{value}\of\rustmetadata \let\rustbenchmarkmachine\pgfplotsretval
\pgfplotstablegetelem{2}{value}\of\rustmetadata \let\rustbenchmarkn\pgfplotsretval
\pgfplotstablegetelem{3}{value}\of\rustmetadata \let\rustbenchmarkiterations\pgfplotsretval

% Command to add benchmark note to figures
\newcommand{\rustbenchmarknote}{\par\vspace{2pt}\noindent{\scriptsize\textit{Benchmarked on \rustbenchmarkmachine\ (\rustbenchmarkdate), $n=\rustbenchmarkn$, \rustbenchmarkiterations\ iterations.}}}
\usetikzlibrary{positioning,arrows.meta,patterns,calc,decorations.pathreplacing,pgfplots.fillbetween}

\geometry{margin=1in}

% Theorem environments
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{claim}[theorem]{Claim}

\title{DeltaSort: Incremental repair of sorted arrays with known updates}
\author{Shubham Dwivedi \\
\small Independent Researcher \\
\small \texttt{shubd3@gmail.com}
}
\date{2026}

\begin{document}
\maketitle

\begin{abstract}
Reading sorted values or records is a fundamental operation in many systems. When records need to be read in a particular order, sorting-on-read incurs repeated $O(n \log n)$ cost which can become a bottleneck in read-heavy systems. A standard solution is to have a derived sorted read-replica that is updated with the latest snapshot asynchronously whenever the system-of-record gets updated. For updating read-replicas, existing approaches rely on either full re-sorting or incremental techniques such as Binary-Insertion-Sort or Extract–Sort–Merge, each providing different time-space tradeoffs. In this paper, we study incremental sorting under an alternative model where the sorting routine is explicitly informed of the indices of updated values since the previous sort. Under this model, we present \emph{DeltaSort}, a new algorithm for incremental sorting that offers distinct time-space trade-off. We present theoretical analysis and experimental evidence that show that \emph{DeltaSort} outperforms existing approaches for practically relevant regimes, while exhibiting clear crossover threshold beyond which other techniques become preferable.
\end{abstract}

%==============================================================================
\section{Introduction}
\label{sec:introduction}
%==============================================================================

Sorting is among the most heavily optimized primitives in modern systems, backed by decades of deep research. Standard library implementations---TimSort~\cite{timsort}, Introsort~\cite{musser1997introspective}, and PDQSort~\cite{peters2021pdqsort} deliver excellent performance for general inputs by exploiting partial order, cache locality, and adaptive strategies. However, these algorithms operate under a \emph{blind} model: they discover structure dynamically rather than being explicitly informed about which values have been updated since the previous sort.

In many practical systems, this assumption is unnecessarily pessimistic. Whenever values in an array are updated, their indices can be easily tracked by using extra space proportional to the number of updates. This is especially true in modern streaming and stateful architectures where updates are processed incrementally and stateful operators maintain metadata about updated records.

As an example, consider an analytics service that materializes and caches query results sorted by a particular field to serve repeat requests efficiently. The underlying data may be distributed across multiple clusters which may see multiple updates, but only a small fraction of those might be relevant for a particular caches query result. A common approach is to invalidate the cached results, which simplifies implementation but forces a full computation the next time, even when updates are sparse.

This pattern also arises in rich client applications. Consider a task management application that shows a large list of tasks sorted by priority or due date. Updates typically affect only a small subset of tasks at a time, such as when user completes a bunch of tasks. In most implementations, these edits trigger a full resort of the list even though the application already knows exactly which tasks were updated. This indicates that there is need for lower-level primitives that treat deltas as a first-class citizen.

Once the indices of the updates are known, a natural question arises: how do we use this extra information and repair the array more efficiently as compared to a full re-sort? Existing approaches (Table~\ref{tab:incremental-sorting-algorithms}) already exploit this extra information, but they force a choice between competing extremes-
\begin{enumerate}
  \item \textbf{Binary-Insertion-Sort (BIS):} For each updated index, remove the value and re-insert it at the correct position using binary search to find the insertion point. This approach uses $O(1)$ extra space, but $O(k n)$ data movement for $k$ updates in an array of size $n$, making it suitable only for very small update batches.
  \item \textbf{Extract–Sort–Merge (ESM):} Extract all updated values into a separate array, sort them using an efficient $O(k \log k)$ algorithm, and then merge them back into the original array. This approach uses $O(n + k \log k)$ time but $O(n)$ extra space even for small $k$, making it suitable only for larger update batches.
\end{enumerate}

This naturally raises the question: are there other intermediate strategies between these two extremes? In this paper, we present such a strategy. Specifically, this paper makes following contributions:

\begin{enumerate}
  \item \textbf{Update-aware sorting model:} We formulate a sorting model in which the sorting routine is explicitly informed of the updated indices since the previous sort. Under this model, Binary-Insertion-Sort and Extract-Sort-Merge serve as the baseline algorithms because they assume knowledge of what has been updated. 

  \item \textbf{DeltaSort algorithm:} We present \emph{DeltaSort}, an update-aware sorting algorithm, which offers a different set of trade-offs (Table~\ref{tab:incremental-sorting-algorithms}) compared to existing approaches. In our experimental evaluation, \emph{DeltaSort} outperforms Binary-Insertion-Sort in execution time for small update batches ($k \ll n$), and provides a more space-efficient alternative to Extract-Sort-Merge for larger update batches ($k = O(n)$).

  \input{figures/incremental-sorting-algorithms}

\end{enumerate}

%==============================================================================
\section{Related Work}
\label{sec:related}
%==============================================================================

Adaptive sorting algorithms exploit existing order in the input to improve performance on nearly sorted data. TimSort~\cite{timsort} and natural merge sort~\cite{knuth1998art} dynamically identify monotonic runs and merge them efficiently, while a substantial body of work formalizes measures of presortedness and analyzes sorting complexity as a function of these measures rather than input size alone~\cite{mannila1985measures}. These approaches, however, assume no explicit knowledge of which values have been updated.

A separate line of work studies incremental computation and view maintenance in database and streaming systems, where updates to input data are propagated to derived results using explicit delta representations~\cite{gupta1995maintenance, nikolic2014incremental, akidau2015dataflow}. These techniques focus on maintaining query results, aggregates, and materialized views, and operate at the level of relational or dataflow operators rather than array-based sorting primitives. They treat deltas as first-class citizen and demonstrate the practical value of update-aware computation. Howevery, they do not specifically address the problem of repairing sorted arrays, which is much more specialized lower-level problem.

Dynamic data structures offer a different trade-off. Self-balancing trees such as AVL trees~\cite{avl1962}, red--black trees~\cite{guibas1978dichromatic}, B-trees~\cite{bayer1972organization}, and skip lists~\cite{pugh1990skip} support efficient ordered updates with logarithmic cost, but abandon contiguous array layout and its associated advantages.

In contrast, this work considers maintaining sorted order in arrays where the indices of updated values since the previous sort are explicitly available. Unlike adaptive sorting algorithms, which need to discover the presortedness, \emph{DeltaSort} already knows which values have been updated and hence can leverage this information to achieve better performance. Unlike incremental view maintenance techniques, which focus on higher-level query results, \emph{DeltaSort} is a low-level sorting primitive for arrays, intended to complement higher-level incremental systems rather than replace them. Unlike dynamic data structures, which sacrifice array layout for update efficiency, \emph{DeltaSort} maintains the contiguous array layout.

%==============================================================================
\section{Problem Model}
\label{sec:model}
%==============================================================================

\begin{definition}[Update-Aware Sorting]
\label{def:problem}
Let $A[0..n-1]$ be an array of size $n$ sorted according to a strict weak ordering defined by a comparator $\texttt{cmp}$. Let $U = \langle u_0, u_1, \dots, u_{k-1} \rangle$ be a sequence of $k$ indices such that \[ 0 \le u_0 < u_1 < \dots < u_{k-1} \le n-1, \] and the values at these indices may have been arbitrarily updated, while values at all other indices remain unchanged. The \emph{update-aware sorting problem} is to restore $A$ to a state that is sorted with respect to $\texttt{cmp}$, given explicit knowledge of the set $U$.
\end{definition}

\begin{remark}
The above model can be easily extended to account for additions and deletions from the array-
\begin{itemize}
  \item \textbf{Addition:} For an existing array of size $n$, append new value to end of the array and add $n$ (the new index) to $U$. 
  \item \textbf{Deletion:} To delete value at index $i$, set its value to a sentinel $\infty$ that is greater than all other values according to $\texttt{cmp}$ and add it to $U$. Once sorted, the array size can be reduced by one removing the last value.
\end{itemize}
\end{remark}

%==============================================================================
\section{DeltaSort Algorithm}
\label{sec:algorithm}
%==============================================================================

\subsection{Overview}

Before we get to how the algorithm works, let's establish useful terminology-

\begin{definition}[Direction]
\label{def:direction}
For an updated index $i$, we define its \emph{direction} based on local order:
\begin{itemize}
  \item \textbf{LEFT ($L$)}: Value \textbf{must} move left---$\texttt{cmp}(A[i-1], A[i]) > 0$ (for $i > 0$).
  \item \textbf{RIGHT ($R$)}: Value \textbf{may} move right and \textbf{cannot} move left.
\end{itemize}
\end{definition}

\begin{remark}
The definition of direction is deliberately asymmetric: L requires a definite violation ($A[i-1] > A[i]$), while R encompasses both violations and non-violations. One might consider a three-way classification with a separate S (stable) category for values already in their correct positions. However, this complicates the setup:
\begin{enumerate}
  \item S values need to be processed identically to R values (both may stay or move right).
  \item Fixing an R value can shift values, potentially converting a previously S neighbor into an R violation. Example: consider array $[20, 30, 10]$ where indexes $0$ and $1$ were updated. Index $0 (20)$ is initially S because it is correctly places w.r.t index $1$ $(30)$, but once index $1$ is fixed, index $0$ becomes an R violation since $20 > 10$. 
\end{enumerate}
The two-way classification avoids this issue by maintaining a useful invariant: \emph{the direction of an updated index never changes until it is repairedd}. This simplifies the implementation.
\end{remark}

\begin{definition}[Segment]
\label{def:segment}
A \emph{segment} is a pair of indices $(i, j)$ with $i < j$ satisfying:
\begin{enumerate}
  \item Either $i = 0$, or $i \in U$ with direction R.
  \item Either $j = n - 1$, or $j \in U$ with direction L.
  \item There do not exist updated indices $p, q \in U$ with $i < p < q < j$ such that $p$ has direction L and $q$ has direction R.
  \item There do not exist updated indices $p < $i and $q > $j such that $(p, q)$ is also a segment.
\end{enumerate}

More intuitively, a segment is a contiguous portion of the array where all R-direction updates precede all L-direction updates. Condition~(3) ensures that no L appears before an R within the segment. Condition~(4) ensures segments are maximal and non-overlapping. Conditions~(1) and~(2) handle boundary cases: a leading segment containing only L's is bounded by the array start, and a trailing segment containing only R's is bounded by the array end. Figure~\ref{fig:segment-structure} illustrates this structure.
\end{definition}

\input{figures/segment-structure}

Armed with these definitions, we can see how DeltaSort works. It operates in two phases:

\begin{enumerate}
  \item \textbf{Phase 1 (Segment):} Extract updated values, sort them, and write back to updated indices in index order. This establishes segments (Definition~\ref{def:segment}) in the array that are disjoint and can be \emph{repaired independently}.
  \item \textbf{Phase 2 (Repair):} Repair each segment left-to-right, deferring R indices to a stack until the first L index is encountered. When a L is encountered, first flush and repair all pending Rs in LIFO order, then repair the L. Continue left-to-right.
\end{enumerate}

Figure~\ref{fig:delta-sort-example} illustrates the full DeltaSort process on a small example.

\input{figures/delta-sort-example}

\subsection{Key Insight: \emph{Segmentation enables localized repair}}
\label{sec:insight}

The key insight behind DeltaSort is that pre-sorting updated values induces a \emph{segmentation} of updates. After Phase~1, updated indices partition into disjoint segments.

\begin{lemma}[Movement Confinement]
\label{lem:confinement}
Value movement during the repair phase is bounded within each segment: no value needs to cross a segment boundary.
\end{lemma}

\begin{proof}
Let $S$ be a segment with R indices $R_0, \ldots, R_{r-1}$ followed by L indices $L_0, \ldots, L_{l-1}$ (where $r \ge 0$ and $l \ge 0$, with $r + l \ge 1$). After Phase~1, updated values are monotonically ordered by index, so $A[R_0] < \cdots < A[R_{r-1}] < A[L_0] < \cdots < A[L_{l-1}]$.

\begin{enumerate}
  \item R values move rightward but cannot pass the first L value $L_0$ (if it exists) or the segment boundary, since $A[R_i] < A[L_0]$ for all $i$.

  \item L values move leftward but cannot pass the last R value $R_{r-1}$ (if it exists) or the segment boundary, since $A[R_{r-1}] < A[L_j]$ for all $j$.
\end{enumerate}

Since no value exits its segment, each segment can be repaired independently. The more segments we have after Phase~1, more localized fixes are possible. Theorem~\ref{thm:movement-bound} establishes an asymptotic bound on number of segments.
\end{proof}

\begin{remark}
  Note that segmentation also opens up opportunities for parallelization since segments can be repaired independently. This has practical implications for improving performance on multi-core systems or distributed environments. To avoid scope creep, we leave parallelization implications for future work.
\end{remark}

\subsection{Pseudocode}

\input{figures/pseudocode}

\subsection{Correctness Proof}

\begin{lemma}[Violation Fix Invariant]
\label{lem:fix-invariant}
Each fix operation during Phase~2 resolves an order violation without introducing new ones.
\end{lemma}

\begin{proof}
We fix each violation using binary search. For binary search to find the correct insertion point, the search range must contain no violations.
\begin{itemize}
    \item \emph{L fix at index $i$}: The search range $[leftBound, i-1]$ contains no L violations because Ls are processed left-to-right, and no R violations because all pending Rs are flushed before any L is fixed.
    \item \emph{R fix at index $i$}: The search range $[i+1, rightBound]$ contains no R violations because Rs are processed in LIFO order with $rightBound$ narrowing after each fix, and no L violations because $rightBound$ never extends past the first L in the segment.
\end{itemize}
\end{proof}

\begin{theorem}[Correctness]
\label{thm:correctness}
DeltaSort produces a correctly sorted array.
\end{theorem}

\begin{proof}
The only violations in the array after Phase~1 are at updated indices. Phase~2 processes each updated index exactly once. By Lemma~\ref{lem:fix-invariant}, each fix resolves a violation without introducing new ones. After all fixes, no violations remain, so the array is sorted.
\end{proof}

\subsection{Complexity Analysis}
\label{sec:complexity}

We analyze the expected total data movement incurred during Phase~2 of DeltaSort under a bounded-range update model. In this model, updated values are drawn independently and uniformly from a fixed value range. This choice ensures that updates do not introduce additional structure beyond what is captured by the directional segmentation created in Phase~1.

\begin{remark}[Choice of Update Model]
\label{rem:update-model}
Alternative models may also be considered. For example, under an unbounded-range model where updated values are drawn from an unrestricted domain, or under a perturbation model where updates introduce small random deviations from existing values, DeltaSort exhibits equal or strictly better expected movement behavior. We focus on the bounded-range model as it is conservative and avoids assuming favorable update structure. A more detailed treatment of alternative models is left to future work.
\end{remark}

\begin{theorem}[Expected Linear Movement]
\label{thm:movement-bound}
Under the bounded-range update model, for an array of size $n$ with $k$ updated indices, the expected total data movement incurred during DeltaSort’s repair phase is $O(n)$, independent of $k$.
\end{theorem}

\begin{proof}[Proof sketch]
Under the bounded-range model, the probability of direction L at the $i$-th updated index is $i/(k-1)$, and R is $(k-1-i)/(k-1)$. Segment boundaries occur at $\mathrm{L}\!\to\!\mathrm{R}$ transitions. By summing the transition probabilities over adjacent pairs, we show that the expected number of such transitions is $\Theta(k)$, yielding $\Theta(k)$ segments. Each segment spans $O(n/k)$ positions in expectation, and by Lemma~\ref{lem:confinement}, movement is confined within segments. Thus, total expected movement is $k \cdot O(n/k) = O(n)$. The full derivation appears in Appendix~\ref{sec:appendix-proof}.
\end{proof}

\begin{theorem}[Comparison Count]
\label{thm:comparison-count}
DeltaSort performs $O(k \log n)$ comparisons.
\end{theorem}

\begin{proof}
Phase~1 sorts the $k$ updated values, requiring $O(k \log k)$ comparisons. In Phase~2, each updated value is repaired using a binary search within its containing segment. By Theorem~\ref{thm:movement-bound}, the expected width of a segment is $O(n/k)$, and searches are confined within segment boundaries. Therefore, each repair requires $O(\log (n/k))$ comparisons, for a total of $O(k \log (n/k))$ comparisons in Phase~2. Summing both phases gives-
\[
O(k \log k) + O(k \log (n/k)) = O\!\bigl(k(\log k + \log (n/k))\bigr) = O(k \log n).
\]
\end{proof}

\begin{theorem}[Space Complexity]
\label{thm:space}
DeltaSort uses $O(k)$ auxiliary space.
\end{theorem}

\begin{proof}
Phase~1 stores $k$ updated indices and $k$ updated values. Phase~2 maintains a pending stack of at most $k$ indices. No $O(n)$ auxiliary structures are required.
\end{proof}

%==============================================================================
\section{Experimental Evaluation}
\label{sec:experiments}
%==============================================================================

\subsection{Setup}

All experiments are conducted using a Rust implementation of DeltaSort~\cite{deltasort-repo} on synthetic datasets of user objects (name, age, country), and executed on an Apple M3 Pro. We use Rust as the primary evaluation language due to its predictable performance characteristics, absence of garbage collection and JIT compilation, and low runtime variance.

Execution times are measured after sufficient warm-up iterations to account for caching and allocator effects. Each reported data point corresponds to the mean over repeated runs, with a 95\% confidence interval of at most $\approx 5\%$. Exact experimental scripts and datasets are provided in the repository for reproducibility~\cite{deltasort-repo}.

We deliberately evaluate DeltaSort under fully random update distributions, which provide no favorable structure to the workload (e.g., bounded-window or perturbed updates). This represents a conservative baseline; structured update patterns would only strengthen DeltaSort’s advantages. Comparisons are performed using a multi-key comparator, reflecting a realistic comparison cost.

DeltaSort is evaluated against the following baselines:

\begin{itemize}
  \item \textbf{NativeSort}: Rust’s \texttt{sort\_by} implementation based on PDQSort~\cite{peters2021pdqsort}, representing full re-sorting of the array. Although not update-aware, NativeSort serves as a critical baseline to identify the crossover threshold beyond which full re-sorting becomes preferable.

  \item \textbf{Binary-Insertion-Sort (BIS)} and \textbf{Extract--Sort--Merge (ESM)}: standard update-aware sorting techniques that represent distinct time--space trade-offs, as described in Section~\ref{sec:introduction}.
\end{itemize}

Together, these baselines provide coverage across the full range of update batch sizes $k$. Table~\ref{tab:complexity} summarizes the theoretical characteristics of each algorithm.

\input{figures/sorting-algorithms}

\subsection{Correctness}
Correctness is formally proven in Theorem~\ref{thm:correctness} and also verified by an extensive set of randomized tests~\cite{deltasort-repo} across various scales and update sizes. The test routine generates a sorted base array of size $n$, applies $k$ random updates at random indices, runs DeltaSort, and asserts that the final array is sorted and contains all original values with updated values.

\subsection{Execution Time}

Figure~\ref{fig:rust-execution-time} shows execution time (in \textmu s) for $n = 100$K values as a function of the number of updated values $k$ on a log--log scale. We use a log--log scale to highlight differences in behavior at small $k$, which is the most practically relevant regime. As $k$ increases, all update-aware algorithms eventually lose to NativeSort (the \emph{crossover threshold}), as the overhead of processing updates begins to dominate.

\input{figures/rust/execution-time}

Several observations emerge from the execution-time profile:

\begin{enumerate}
  \item \textbf{Asymptotic trend is consistent.} For larger values of $k$, the observed execution times for all algorithms closely follow their expected asymptotic trends. In particular, BIS exhibits superlinear growth consistent with its $O(kn)$ movement cost, while ESM and NativeSort converge toward $O(n \log n)$ behavior, respectively.

  \item \textbf{NativeSort performs well for very small updates.} Although NativeSort (PDQSort~\cite{peters2021pdqsort}) is blind to updates, it effectively exploits presortedness for extremely small $k$, achieving up to $\approx 10\times$ better performance for $k \lessapprox 1\%$.

  \item \textbf{Binary-Insertion-Sort degrades rapidly.} BIS outperforms ESM and NativeSort for very small update sizes ($k \lessapprox 0.01\%$) but quickly becomes the worst-performing algorithm as $k$ increases due to its $O(kn)$ data movement cost.

  \item \textbf{Extract--Sort--Merge dominates the mid-range.} ESM is the fastest baseline across most of the intermediate range ($1\% \lessapprox k \lessapprox 80\%$), provided sufficient memory is available. Its performance comes at the cost of $O(n)$ auxiliary space.

  \item \textbf{DeltaSort occupies a narrow but meaningful regime.} DeltaSort outperforms both BIS and ESM for $k \lessapprox 1\%$. It uses $O(k)$ auxiliary space—more than BIS ($O(1)$) but substantially less than ESM ($O(n)$). While this regime is small in absolute terms, it aligns well with practical workloads where deltas are usually a small percentage of the full dataset.
\end{enumerate}

These observations suggest that, much like hybrid blind sorting algorithms (e.g., TimSort~\cite{timsort}, PDQSort~\cite{peters2021pdqsort}), it is beneficial to employ \emph{hybrid update-aware} strategies that adapt to update size and execution environment. For the Rust implementation evaluated here, an optimal strategy for an environment with no memory constraints could be:

\begin{enumerate}
  \item Use DeltaSort for $k \lessapprox 1\%$.
  \item Use ESM for $1\% \lessapprox k \lessapprox 80\%$.
  \item Fall back to NativeSort for $k \gtrapprox 80\%$.
\end{enumerate}

The precise crossover thresholds depend on the execution environment, data distribution, and comparator cost, and should be treated as indicative rather than universal. We will explore these hybrid strategies further in future work.

\subsection{Comparison Count}

Figure~\ref{fig:rust-comparator-count} shows the total number of comparisons as a function of update count $k$. We have used a realistic multi-key comparator that compares user objects by (country, age, name) to reflect practical comparison costs.

\input{figures/rust/comparator-count}

Several clear patterns emerge:

\begin{enumerate}
  \item \textbf{Asymptotic behavior matches theory.} The observed comparator counts closely follow the  expected asymptotic trends for all algorithms across the full range of $k$.

  \item \textbf{ESM incurs high comparison cost even for small updates.} Due to the full merge step, ESM performs a large number of comparisons even when $k$ is small, which explains its weaker performance when the comparator is expensive.

  \item \textbf{DeltaSort matches BIS asymptotically.} DeltaSort's comparator count closely tracks BIS for $k \lessapprox 1\%$ and remains within $\approx 10$--$20\%$ of BIS for larger $k$, while preserving the same $O(k \log n)$ asymptotic behavior. Note that for large $k$, even though BIS is most comparison-efficient, its data movement cost makes it an impractical choice.
\end{enumerate}

The key takeaway is that DeltaSort, like BIS, substantially reduces redundant comparison work relative to ESM and NativeSort. As comparator cost increases, this advantage becomes more pronounced. For small update ranges, where BIS and DeltaSort have similar comparison counts, DeltaSort’s movement efficiency becomes the dominant factor, and its relative advantage over BIS is expected to persist across a wide range of comparator costs.

\subsection{Crossover Threshold}

Every update-aware algorithm would eventually be beaten by NativeSort as $k$ increases, once the overhead of processing the updates outweighs its benefits. For each algorithm, we calculate the threshold $k_c$ via binary search across the range of $k$. Figure~\ref{fig:rust-crossover-all} shows the crossover thresholds for all three update-aware algorithms versus NativeSort.

Several patterns emerge:

\begin{enumerate}
  \item \textbf{BIS has a very low crossover threshold.} Due to its $O(kn)$ data movement cost, BIS is competitive only for very small update sizes.

  \item \textbf{ESM achieves the highest crossover threshold.} ESM remains faster than NativeSort up to $k \approx 60$--$80\%$, consistent with its $O(n + k \log k)$ time complexity, which approaches full sorting only when $k$ is large.

  \item \textbf{DeltaSort exhibits an intermediate threshold.} DeltaSort’s crossover point lies between BIS and ESM, aligning with its $O(k \log n)$ time complexity, which interpolates between the two extremes.

  \item \textbf{Thresholds are weakly dependent on array size.}
  Crossover points are largely stable across array sizes, though we observe a decline for large arrays ($n \gtrapprox 200$K). This may be due to cache effects or memory allocation overheads. We will study this behavior in more detail in future work.
\end{enumerate}

The key takeaway is that DeltaSort occupies a well-defined middle ground among update-aware algorithms. It supports substantially larger update batches than BIS while requiring significantly less auxiliary space than ESM, making it attractive in regimes where both time and space constraints matter.

\input{figures/rust/crossover-all}

An interesting trend in Figure~\ref{fig:rust-execution-time} is that for update sizes up to $\approx 1\%$, DeltaSort consistently outperforms ESM while using significantly less auxiliary space, making it a strictly better choice in this regime. To examine this boundary more closely, we plot the direct crossover between DeltaSort and ESM in Figure~\ref{fig:rust-crossover-ds-vs-esm}.

\input{figures/rust/crossover-ds-vs-esm}

Figure~\ref{fig:rust-crossover-ds-vs-esm} shows that there exists a non-trivial regime in which DeltaSort dominates ESM both in execution time and space usage. For example, for an array of size $n = 100$K, DeltaSort outperforms ESM for up to $\approx 1K$ updates, while using only $\approx 1\%$ of the auxiliary space required by ESM. As array size increases, this regime steadily shrinks. For large arrays, the crossover threshold moves toward smaller fractions, indicating that DeltaSort loses to ESM earlier. Although both algorithms incur $O(n)$ data movement in the worst case, this trend suggests that the constant factors associated with DeltaSort’s movement become more pronounced at larger scales. As a result, ESM’s simpler linear merge begins to dominate for smaller $k$ as $n$ grows.

\subsection{Performance in managed execution environments}

DeltaSort was also implemented in JavaScript~\cite{deltasort-repo} and benchmarked on Node v22 to evaluate behavior in managed runtimes to study how factors such as garbade collections, JIT compilation and runtime optimizations impact performance. Figure ~\ref{fig:js-execution-time} shows execution time for $n = 100$K as a function of update size $k$.

\input{figures/js/execution-time}

Several observations stand out:

\begin{enumerate}
  \item \textbf{DeltaSort and BIS largely overlap for small updates.} For $k \lessapprox 0.1\%$, even though BIS and DeltaSort beat ESM and NativeStore, DeltaSort does not exhibit a clear advantage over Binary-Insertion-Sort. Unlike in Rust, batching updates does not help improve performance meaningfully in this regime.

  \item \textbf{ESM beats NativeSort.} Extract--Sort--Merge performs better than NativeSort up to $\approx 50\%$ update batch size.

  \item \textbf{DeltaSort’s advantage diminishes in managed runtimes.}
  JavaScript runtimes like V8 do not guarantee contiguous array layout~\cite{v8elementskinds}, and movement costs are not proportional to the number of shifted elements. As a result, DeltaSort’s core optimization---reducing physical data movement through segmentation---does not translate into consistent wall-clock improvements.
\end{enumerate}

The key takeaway is that \emph{DeltaSort’s performance benefits rely on a predictable movement cost model}, which holds in low-level, unmanaged environments (e.g., Rust) but not in managed runtimes such as V8. In such environments, the practical strategy simplifies to:
\begin{itemize}
  \item Use Binary-Insertion-Sort for very small updates. There is no measurable benefit of using DeltaSort.
  \item Prefer Extract--Sort--Merge or NativeSort for larger update batches.
\end{itemize}

These results highlight that update-aware sorting algorithms must be evaluated together with the execution semantics of the target runtime, rather than assuming uniform cost models across environments. Appendix ~\ref{sec:appendix-js} provides additional data about crossover thesholds in JS.

%==============================================================================
\section{Future Work}
\label{sec:future}
%==============================================================================

This work opens up several directions for further investigation.

First, while our analysis focuses on a bounded-range update model with uniformly random updates, many practical workloads exhibit additional structure. Examples include bounded-window updates, value perturbation models, and temporally correlated updates. These models may further reduce the expected movement and comparison costs of DeltaSort, potentially expanding the regime in which it outperforms existing techniques. A more systematic study of such update models would help characterize when update-aware sorting provides the greatest benefit.

Second, DeltaSort’s repair process naturally decomposes the array into independent segments. This segmentation suggests opportunities for parallel execution, where different segments could be repaired concurrently without interference. In contrast, Binary-Insertion-Sort is inherently sequential due to overlapping in-place shifts, and Extract–Sort–Merge parallelizes only partially while incurring substantial auxiliary space costs. Exploring parallel variants of DeltaSort and understanding their scalability on multi-core and NUMA systems is a promising direction for future work.

Third, our experimental results reveal that crossover thresholds between update-aware algorithms and full re-sorting depend on array size, execution environment, and memory behavior. In particular, we observe a decline in DeltaSort’s advantage for very large arrays, likely due to cache effects and data movement overheads. A deeper investigation into these effects, including cache-aware and memory-bandwidth-aware models, could lead to improved implementations or refined hybrid strategies.

Finally, while this paper treats DeltaSort as a standalone algorithm, practical systems will benefit from adaptive hybrid strategies that select among Binary-Insertion-Sort, DeltaSort, Extract–Sort–Merge, and full re-sorting based on observed update sizes and system constraints. Designing heuristic-based, low-overhead mechanisms for such dynamic selection remains an open challenge.

%==============================================================================
\section{Conclusion}
\label{sec:conclusion}
%==============================================================================

This paper studied the problem of maintaining sorted arrays under incremental \emph{known} updates. We argued that this \emph{update-aware} setting arises naturally in many practical systems, yet is not directly addressed by standard blind sorting routines. Within this model, we presented \emph{DeltaSort}, an incremental sorting algorithm that batches multiple updates instead of applying them independently. The key idea is to exploit structure created by pre-sorting updated values, which induces directional segmentation and enables localized, non-overlapping fixes. This allows DeltaSort to reduce redundant data movement.

Our theoretical analysis shows that DeltaSort matches the comparison efficiency of Binary-Insertion-Sort while providing stronger guarantees on data movement under reasonable update models. Experimental results in Rust demonstrate that DeltaSort occupies a distinct space in the incremental sorting design spectrum: it outperforms Binary-Insertion-Sort for small update batches, is more space-efficient than Extract–Sort–Merge across a wider range of updates, and exhibits clear crossover points beyond which other strategies become preferable. Importantly, the results also highlight the limits of this approach. DeltaSort does not dominate existing techniques across all kinds of execution environments, and its advantages depend on predictable memory movement costs. In managed runtimes and environments with less transparent memory layouts, these benefits may diminish.

Overall, this work shows that exposing update information to the sorting routine enables new algorithmic trade-offs that are not available to blind sorting algorithms. DeltaSort demonstrates that even within the well-studied domain of sorting, modest changes to the problem formulation can yield practically useful algorithmic techniques for incremental workloads.

%==============================================================================
\bibliographystyle{plain}
\bibliography{refs}

\appendix

\section{Full Proof of Expected Linear Movement}
\label{sec:appendix-proof}

We provide the complete derivation for Theorem~\ref{thm:movement-bound}.

\begin{proof}
Consider the sequence of directions induced by the $k$ updated indices after Phase~1. Let position $i \in \{0, \ldots, k-1\}$ denote the $i$-th updated index. Under the bounded-range update model, an updated value is equally likely to fall anywhere within the global value range. The probability that position $i$ has direction L is proportional to the number of preceding updates, and R proportional to the number following:
\[
\Pr[\text{L at position } i] = \frac{i}{k-1},
\qquad
\Pr[\text{R at position } i] = \frac{k-1-i}{k-1}.
\]

Segment boundaries occur at $\mathrm{L}\!\to\!\mathrm{R}$ transitions. For adjacent positions $(i, i+1)$:
\[
\Pr[\mathrm{LR} \text{ at } (i, i+1)] = \frac{i}{k-1} \cdot \frac{k-2-i}{k-1} = \frac{i(k-2-i)}{(k-1)^2}.
\]

Summing over all adjacent pairs:
\[
\mathbb{E}[\mathrm{LR\ transitions}] = \sum_{i=0}^{k-2} \frac{i(k-2-i)}{(k-1)^2} = \frac{1}{(k-1)^2} \sum_{i=0}^{k-2} i(k-2-i).
\]

Using the identity $\sum_{i=0}^{m} i(m-i) = \frac{m(m+1)(m-1)}{6}$ with $m = k-2$:
\[
\sum_{i=0}^{k-2} i(k-2-i) = \frac{(k-2)(k-1)(k-3)}{6},
\]
and therefore:
\[
\mathbb{E}[\mathrm{LR\ transitions}] = \frac{(k-2)(k-3)}{6(k-1)} = \Theta(k).
\]

The expected number of segments is $1 + \Theta(k) = \Theta(k)$. Under uniform index distribution, each segment spans $O(n/k)$ positions. By Lemma~\ref{lem:confinement}, movement is confined within segments, so:
\[
\mathbb{E}[\text{total movement}] \le k \cdot O(n/k) = O(n). \qedhere
\]
\end{proof}

\section{Crossover Thresholds in JavaScript}
\label{sec:appendix-js}

Figures~\ref{fig:js-crossover-all} and~\ref{fig:js-crossover-ds-vs-esm} show crossover thresholds for the JavaScript implementation.

\input{figures/js/crossover-all}

\input{figures/js/crossover-ds-vs-esm}

\paragraph{Observations.} In JavaScript, crossover thresholds are generally lower than in Rust due to the managed runtime's memory movement behavior. BIS and DeltaSort have almost identical behavior in JavaScript, while ESM maintains a higher threshold ($\approx 35$--$40\%$). This implies that for small $k$, BIS is preferable to DeltaSort in JavaScript because of lower space requirement and simpler implementation, unlike in Rust where DeltaSort has a clear measurable advantage.

%==============================================================================
\end{document}

