\documentclass[11pt]{article}

\usepackage{amsmath,amssymb}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{geometry}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{placeins}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning,arrows.meta,patterns,calc,decorations.pathreplacing}

\geometry{margin=1in}

% Theorem environments
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\title{DeltaSort: Incremental repair of sorted arrays with known updates}
\author{Shubham Dwivedi \\
\small Independent Researcher \\
\small \texttt{shubd3@gmail.com}
}
\date{January 2026}

\begin{document}
\maketitle

\begin{abstract}
Maintaining sorted order under incremental updates is a common requirement in read-heavy systems,
yet most production sorting routines are blind to which elements have changed and must repeatedly
rediscover partial order from scratch. This paper explores an alternative model in which the sorting
routine is explicitly informed of the indices updated since the previous sort. Under this model,
we present \emph{DeltaSort}, an incremental repair algorithm for arrays that batches multiple
updates instead of applying them independently. \emph{DeltaSort} reduces redundant comparison work
and overlapping data movement by exploiting update locality and batching. An experimental evaluation
of a Rust implementation shows that \emph{DeltaSort} consistently outperforms repeated binary-insertion
and blind native sorting for update batch sizes up to ~25\%, with multi-fold speedups and a clear
crossover point depending on array size where full re-sorting becomes preferable. These results
suggest that tighter integration between update pipelines and sorting routines can yield significant
performance gains in real incremental-sorting workloads.
\end{abstract}

%==============================================================================
\section{Introduction}
%==============================================================================

Sorting is among the most heavily optimized primitives in modern systems. Standard library
implementations—TimSort~\cite{timsort}, Introsort~\cite{musser1997introspective}, and
PDQSort~\cite{peters2021pdqsort}—deliver excellent performance for general inputs by
exploiting partial order, cache locality, and adaptive strategies. However, these algorithms
operate under a \emph{blind} model: they rediscover structure dynamically rather than being
explicitly informed which elements have changed since the previous sort.

In many practical systems, this assumption is unnecessarily pessimistic. Sorted arrays are
often maintained incrementally in read-heavy workloads where updates affect only a small
subset of elements and the indices of those updates are already known. Nevertheless, this
information is typically discarded, and systems fall back to blind re-sorting or independent
incremental repairs. To address these limitations, this paper makes the following
contributions:

\begin{enumerate}
  \item \textbf{Update-aware sorting model.}
  We formulate an incremental sorting model in which the sorting routine is explicitly
  informed of the indices updated since the previous sort. Under this model, a natural
  baseline is repeated binary insertion, where each update is repaired independently using
  binary search. While correct, this approach exhibits efficiency limitations for batched
  updates due to repeated full-range searches and overlapping data movement.

  \item \textbf{DeltaSort algorithm.}
  We present \emph{DeltaSort}, a coordinated incremental repair algorithm for sorted arrays
  designed for the update-aware model. DeltaSort batches updates and repairs them jointly,
  reducing redundant comparison work and overlapping data movement by exploiting update
  locality and batching. DeltaSort preserves correctness and does not improve asymptotic
  complexity, but achieves consistent practical performance improvements over repeated
  binary insertion and blind native sorting in relevant update regimes.
\end{enumerate}

%==============================================================================
\section{Related Work}
\label{sec:related}
%==============================================================================

Adaptive sorting algorithms exploit existing order in the input to improve performance on
nearly sorted data. TimSort~\cite{timsort} and natural merge sort~\cite{knuth1998art} identify
monotonic runs dynamically and merge them efficiently, yielding improved performance when
such structure is present. A substantial body of work formalizes measures of presortedness
and studies sorting algorithms whose complexity depends on these measures rather than input
size alone~\cite{mannila1985measures}. These approaches, however, must discover structure
through full-array scans and do not assume explicit knowledge of which elements were modified.

Dynamic data structures offer a different trade-off. Self-balancing trees such as AVL
trees~\cite{avl1962}, red--black trees~\cite{guibas1978dichromatic}, B-trees~\cite{bayer1970organization},
and skip lists~\cite{pugh1990skip} support efficient ordered updates with logarithmic cost,
but sacrifice contiguous memory layout and cache locality. Library sort~\cite{bender2006insertion}
reduces insertion cost by maintaining gaps in the array, but addresses online insertion and
incurs additional space overhead.

In contrast, this work focuses on maintaining sorted order in arrays under batched updates
where the indices of modified elements are explicitly available. This update-aware model
enables efficient repair without auxiliary data structures or additional memory, and
distinguishes DeltaSort from prior adaptive and incremental approaches.

%==============================================================================
\section{Problem Model}
\label{sec:model}
%==============================================================================

\begin{definition}[Update-Aware Sorting]
\label{def:problem}
Let $A[0..n-1]$ be an array sorted according to a strict weak ordering
defined by a comparator $\texttt{cmp}$. Suppose a set of indices
$D = \{d_1, \ldots, d_k\} \subseteq \{0, \ldots, n-1\}$ is given such that
the values at these indices may have been arbitrarily modified, while
values at all other indices remain unchanged.

The \emph{update-aware sorting problem} is to restore $A$ to a state that
is sorted with respect to $\texttt{cmp}$, given explicit knowledge of the
set $D$.
\end{definition}

%==============================================================================
\section{DeltaSort Algorithm}
\label{sec:algorithm}
%==============================================================================

\subsection{Overview}

DeltaSort operates in two phases:
\begin{enumerate}
  \item \textbf{Phase 1 (Preparation):} Extract dirty values, sort them, write back to
        dirty positions in index order. This establishes monotonicity among dirty elements.
  \item \textbf{Phase 2 (Repair):} Process dirty indices left-to-right, immediately
        repairing LEFT violations while deferring RIGHT violations to a stack. After
        the left-to-right pass, flush the stack in LIFO order to process RIGHT violations.
\end{enumerate}

\input{figures/algorithm}

\subsection{Key Insight: Directional Segmentation}
\label{sec:insight}

The central insight behind DeltaSort is that pre-sorting the dirty values
induces a segmentation of updates by \emph{direction of movement}. After
Phase~1, each dirty element either violates order with its left neighbor
(LEFT), violates order with its right neighbor (RIGHT), or is already
locally ordered (STABLE). Moreover, adjacent dirty elements with the same
direction form contiguous directional segments.

This segmentation enables coordinated repair. LEFT-moving elements can be
processed immediately in a left-to-right scan using progressively narrowed
search ranges, since all elements to their left are already stable. In
contrast, RIGHT-moving elements cannot be safely placed until elements to
their right have been handled. DeltaSort therefore defers RIGHT segments
using a stack and processes them in LIFO order after the scan completes.

Stack-based processing ensures that when a RIGHT-moving element is repaired,
the impacted segment is already fixed. This coordination avoids overlapping
data movement and redundant search.

\subsection{Why Phase 1 Creates Segments}

\begin{example}[Benefit of Pre-sorting]
\label{ex:presort}
Consider array $[1, 8, 5, 2, 9]$ with dirty indices $\{1, 3\}$ (original sorted array
was $[1, 3, 5, 7, 9]$, positions 1 and 3 were updated to 8 and 2).

With standard binary insertion (extract-then-insert), the algorithm would:
\begin{itemize}
  \item Extract dirty values $\{8, 2\}$, leaving placeholders or compacting
  \item Insert each back via binary search over the \emph{entire} remaining array
\end{itemize}
This is correct but each insertion searches the full range.
\end{example}

Phase 1 sorts $\{8, 2\} \to \{2, 8\}$ and writes to positions $\{1, 3\}$, yielding
$[1, 2, 5, 8, 9]$. This establishes \emph{monotonicity} among dirty values, which has
two critical consequences:

\begin{enumerate}
  \item \textbf{Bounded search ranges}: Once a LEFT-moving element settles at position $t$,
        subsequent LEFT searches need only consider $[t+1, ...]$.
  \item \textbf{Directional coherence}: Dirty elements naturally form segments where all
        elements in a segment move in the same direction, enabling coordinated processing.
\end{enumerate}

\begin{lemma}[Phase 1 Establishes Monotonicity]
\label{lem:monotonicity}
After Phase 1, for any dirty indices $d_i < d_j$, it holds that $\texttt{cmp}(A[d_i], A[d_j]) \leq 0$.
\end{lemma}

\begin{proof}
Phase 1 sorts dirty values and assigns the $i$-th smallest value to the $i$-th smallest
index. Thus $A[d_i] \leq A[d_j]$ for $d_i < d_j$.
\end{proof}

\begin{remark}[Bounded Search via \texttt{leftBound}]
\label{rem:leftbound}
The implementation maintains a \texttt{leftBound} variable that advances after each
LEFT move. When a dirty element moves from index $i$ to position $t < i$, the algorithm sets
$\texttt{leftBound} = t + 1$. Subsequent LEFT searches only consider $[\texttt{leftBound}, i-1]$,
not $[0, i-1]$. This progressively narrows the search range as LEFT-moving elements
are processed.
\end{remark}

The monotonicity established by Phase 1 enables DeltaSort's efficiency through segmentation:
\begin{enumerate}
  \item \textbf{Directional segments}: After Phase 1, dirty elements form contiguous segments
        where all elements in a segment have the same direction (LEFT, RIGHT, or STABLE).
        Figure~\ref{fig:segments} illustrates this segmentation.
  \item \textbf{Stack-based coordination}: RIGHT-moving elements are pushed to a stack and
        processed in LIFO order. This ensures that when processing element at index $s$,
        all elements at indices $> s$ have already been processed, so $s$'s target is stable.
  \item \textbf{Progressive search narrowing}: The \texttt{leftBound} variable ensures
        that LEFT searches become progressively narrower as elements are processed.
  \item \textbf{STABLE detection}: After pre-sorting, some dirty elements may already
        be in their correct position relative to neighbors, requiring no movement.
\end{enumerate}

\begin{example}[Movement Cancellation]
\label{ex:cancellation}
Consider array $[1, 3, 5, 7, 9]$ with dirty indices $\{1, 3\}$ receiving values $8$ and $2$
respectively, yielding $[1, 8, 5, 2, 9]$.

\textbf{Binary Insertion} (extract-then-insert):
\begin{itemize}
  \item Extract values at indices $3, 1$ (descending): array becomes $[1, 5, 9]$
  \item Insert $8$: finds position $2$, array becomes $[1, 5, 8, 9]$ (9 shifts right)
  \item Insert $2$: finds position $1$, array becomes $[1, 2, 5, 8, 9]$ (5, 8, 9 shift right)
\end{itemize}
Clean element $5$ shifted during extraction and again during insertion.

\textbf{DeltaSort}:
\begin{itemize}
  \item Phase 1: Sort $\{8, 2\} \to \{2, 8\}$, write to indices $\{1, 3\}$: $[1, 2, 5, 8, 9]$
  \item Phase 2: Check directions---both elements are now STABLE!
\end{itemize}
Clean element $5$ never moved. The pre-sorting phase reassigned values such that both
dirty elements landed in positions where they satisfy ordering constraints immediately.
\end{example}

\input{figures/cancellation}

\subsection{Detailed Algorithm}

\begin{algorithm}[t]
\caption{DeltaSort}
\label{alg:deltasort}
\begin{algorithmic}[1]
\Require Array $A[0..n-1]$, dirty indices $D$, comparator $\texttt{cmp}$
\Ensure $A$ is sorted
\Statex
\State \textbf{Phase 1: Establish Monotonicity}
\State $\texttt{dirty} \gets \text{sort}(D)$ \Comment{Sort indices ascending}
\State $\texttt{values} \gets [A[d] : d \in \texttt{dirty}]$
\State $\texttt{values} \gets \text{sort}(\texttt{values}, \texttt{cmp})$
\For{$i \gets 0$ \textbf{to} $|\texttt{dirty}| - 1$}
    \State $A[\texttt{dirty}[i]] \gets \texttt{values}[i]$
\EndFor
\Statex
\State \textbf{Phase 2: Process Left-to-Right}
\State $\texttt{stack} \gets []$; $\texttt{leftBound} \gets 0$
\For{$p \gets 0$ \textbf{to} $|\texttt{dirty}| - 1$}
    \State $i \gets \texttt{dirty}[p]$
    \State $d \gets \Call{Direction}{A, i}$
    \If{$d = \texttt{LEFT}$}
        \State \Call{FlushStack}{$\texttt{stack}, i-1$} \Comment{Flush before processing LEFT}
        \State $t \gets \Call{BinarySearchLeft}{A, A[i], \texttt{leftBound}, i-1}$
        \State \Call{Move}{A, i, t}
        \State $\texttt{leftBound} \gets t + 1$
    \Else
        \State $\texttt{stack.push}(i)$
    \EndIf
\EndFor
\Statex
\State \textbf{// Flush remaining RIGHT violations}
\State \Call{FlushStack}{$\texttt{stack}, n-1$}
\end{algorithmic}
\end{algorithm}

\begin{algorithmic}[1]
\Function{Direction}{$A$, $i$}
    \If{$i > 0 \land \texttt{cmp}(A[i-1], A[i]) > 0$}
        \State \Return \texttt{LEFT} \Comment{Violates order with left neighbor}
    \ElsIf{$i < n-1 \land \texttt{cmp}(A[i], A[i+1]) > 0$}
        \State \Return \texttt{RIGHT} \Comment{Violates order with right neighbor}
    \Else
        \State \Return \texttt{STABLE}
    \EndIf
\EndFunction
\end{algorithmic}

\vspace{0.5em}

\begin{algorithmic}[1]
\Function{FlushStack}{$\texttt{stack}$, $\texttt{rightBound}$}
    \While{$\texttt{stack} \neq \emptyset$}
        \State $s \gets \texttt{stack.pop}()$ \Comment{Process in LIFO order}
        \If{$\Call{Direction}{A, s} = \texttt{RIGHT}$}
            \State $t \gets \Call{BinarySearchRight}{A, A[s], s+1, \texttt{rightBound}}$
            \State \Call{Move}{A, s, t}
        \EndIf
    \EndWhile
\EndFunction
\end{algorithmic}

\vspace{0.5em}

\begin{algorithmic}[1]
\Function{Move}{$A$, $from$, $to$}
    \State $v \gets A[from]$
    \If{$from < to$}
        \State Shift $A[from+1..to]$ left by one
    \ElsIf{$from > to$}
        \State Shift $A[to..from-1]$ right by one
    \EndIf
    \State $A[to] \gets v$
\EndFunction
\end{algorithmic}

\subsection{Design Rationale}

\paragraph{Why flush before LEFT?}
When a LEFT-moving dirty element is encountered at index $i$, any RIGHT-moving elements
on the stack (with indices $< i$) must be processed first. Otherwise, moving the LEFT
element could shift the RIGHT elements' positions, invalidating their stack indices.

\paragraph{Why LIFO order?}
Stack elements are pushed in ascending index order. LIFO processing (highest index first)
ensures that when the element at index $s$ is moved, elements at lower indices haven't shifted
yet, so their stack indices remain valid.

\paragraph{Why re-check Direction?}
After Phase 1 writes or after flushing, an element's direction may change (a STABLE
element might become RIGHT, or a RIGHT element might become STABLE). The re-check
avoids unnecessary moves.

\subsection{Correctness Proof}

\begin{definition}[Violation]
A \emph{violation} at index $i$ exists if $\texttt{cmp}(A[i-1], A[i]) > 0$ (for $i > 0$)
or $\texttt{cmp}(A[i], A[i+1]) > 0$ (for $i < n-1$).
\end{definition}

\begin{lemma}[Violations Only at Dirty Boundaries]
\label{lem:violations}
After Phase 1, violations can only exist at boundaries involving dirty indices.
Specifically, a violation at position $i$ implies $i \in D$ or $i+1 \in D$.
\end{lemma}

\begin{proof}
Clean elements retain their values from the original sorted array. For two adjacent
clean elements at positions $i$ and $i+1$, neither has changed, so
$\texttt{cmp}(A[i], A[i+1]) \leq 0$ (sorted order preserved). Violations only occur
where a dirty element is adjacent to another element with which it violates order.
\end{proof}

\begin{lemma}[Move Correctness]
\label{lem:move-correct}
After $\Call{Move}{A, i, t}$ where $t = \Call{BinarySearchLeft/Right}{\ldots}$:
\begin{enumerate}
  \item The element originally at $A[i]$ is now at position $t$
  \item $\texttt{cmp}(A[t-1], A[t]) \leq 0$ (if $t > 0$)
  \item $\texttt{cmp}(A[t], A[t+1]) \leq 0$ (if $t < n-1$ and within search bounds)
\end{enumerate}
\end{lemma}

\begin{proof}
Binary search finds the correct insertion point by comparing with neighbors. The
element is placed where it satisfies order constraints with its new neighbors.
\end{proof}

\begin{lemma}[Stack Index Validity]
\label{lem:stack-validity}
When an index $s$ is popped from the stack and processed, $s$ still refers to the
same element that was pushed.
\end{lemma}

\begin{proof}
Consider when $s$ was pushed: it was a dirty index with direction RIGHT or STABLE.
Between push and pop, two types of operations can occur:

\emph{(a) LEFT repairs}: These move elements at index $i > s$ to positions $t \leq i - 1$.
Since $s < i$, the region $[t, i-1]$ is to the right of $s$; elements at $s$ don't shift.

\emph{(b) RIGHT repairs from stack flushes}: These process indices in LIFO order.
When $s$ is popped, all indices greater than $s$ have already been processed. Their
movements shift elements to the right, in regions $[s'+1, t']$ where $s' > s$. Since
$s < s'$, position $s$ is unaffected.

In both cases, the element at index $s$ hasn't moved, so $s$ remains valid.
\end{proof}

\begin{theorem}[Correctness]
\label{thm:correctness}
DeltaSort produces a correctly sorted array.
\end{theorem}

\begin{proof}
By Lemma~\ref{lem:violations}, after Phase 1, violations only involve dirty indices.
Figure~\ref{fig:segments} illustrates how the array can be viewed as segments where
dirty elements move within bounded regions without crossing segment boundaries.
Each dirty index is processed exactly once in Phase 2:
\begin{itemize}
  \item If Direction is LEFT: processed immediately with Move
  \item If Direction is RIGHT or STABLE: pushed to stack, later processed (if still RIGHT)
\end{itemize}

By Lemma~\ref{lem:stack-validity}, stack indices remain valid when processed.
By Lemma~\ref{lem:move-correct}, each Move places the element correctly.
After all dirty indices are processed, no violations remain (all were fixed), so
the array is sorted.
\end{proof}

\subsection{Algorithm Analysis}

\begin{theorem}[Time Complexity]
\label{thm:time}
DeltaSort runs in $O(k \log k + k \log n + M)$ time, where $M$ is total movement.
\end{theorem}

\begin{proof}
\textbf{Phase 1}: Sort $k$ indices: $O(k \log k)$. Sort $k$ values: $O(k \log k)$.
Write back: $O(k)$.

\textbf{Phase 2}: Each dirty index: $O(1)$ direction check, $O(\log n)$ binary search.
Total: $O(k \log n)$. Movement: $O(M)$.
\end{proof}

\begin{theorem}[Space Complexity]
\label{thm:space}
DeltaSort uses $O(k)$ auxiliary space.
\end{theorem}

\begin{theorem}[Comparison Optimality]
\label{thm:optimal-comparisons}
DeltaSort achieves $O(k \log n)$ comparisons, which matches the information-theoretic
lower bound: each of $k$ dirty elements can occupy any of $n$ final positions, requiring
$\log_2(n^k) = k \log n$ comparisons to distinguish all configurations.
\end{theorem}

\begin{remark}[Movement Efficiency]
While worst-case movement is $O(kn)$, the segmentation created by Phase 1 tends to
reduce movement in practice. When dirty values would otherwise ``cross'' (one moving left,
another right over the same positions), Phase 1 reassigns them to minimize displacement.
Empirical results in \S\ref{sec:experiments} demonstrate substantial speedups, validating
that movement is typically much less than the worst case.
\end{remark}

\begin{table}[h]
\centering
\caption{Algorithm complexity comparison.}
\label{tab:complexity}
\begin{tabular}{l c c c c}
\toprule
Algorithm & Comparisons & Movement & Space & Bounded Search? \\
\midrule
Native Sort & $O(n \log n)$ & $O(n \log n)$ & $O(n)$ & N/A \\
Binary Insertion & $O(k \log n)$ & $O(kn)$ & $O(1)$ & No \\
Extract-Sort-Merge & $O(k \log k + n)$ & $O(n)$ & $O(n)$ & N/A \\
\textbf{DeltaSort} & $O(k \log n)$ & $O(kn)$* & $O(k)$ & Yes \\
\bottomrule
\end{tabular}

\vspace{0.3em}
{\small *Worst case; typically $O(k \cdot \bar{m})$ for average movement distance $\bar{m}$.}
\end{table}

%==============================================================================
\section{Experimental Evaluation}
\label{sec:experiments}
%==============================================================================

\subsection{Baseline Algorithms}

\paragraph{Native Sort.}
Re-sort the entire array: $O(n \log n)$ comparisons, $O(n \log n)$ movements.

\paragraph{Binary Insertion (BI).}
For each $d \in D$: extract all dirty values, then for each: binary search for correct
position, reinsert. Cost: $O(k \log n)$ comparisons, $O(kn)$ worst-case movement.
Always correct, but searches the full array range for each insertion.

\paragraph{Extract-Sort-Merge (ESM).}
Extract dirty values, sort them, merge with clean elements.
Cost: $O(k \log k + n)$ comparisons, $O(n)$ movement.
Always correct but requires $O(n)$ auxiliary space.

\subsection{Setup}

\paragraph{Primary Implementation.} Rust 1.75 (release build with optimizations).

\paragraph{Hardware.} Apple M2 Pro, 16GB RAM, macOS 14.

\paragraph{Algorithms.}
\begin{itemize}
  \item \textbf{Native}: Rust's \texttt{sort\_by} (pattern-defeating quicksort)
  \item \textbf{BI}: Binary Insertion (extract-then-insert)
  \item \textbf{ESM}: Extract-Sort-Merge
  \item \textbf{DS}: DeltaSort
\end{itemize}

\paragraph{Data.} User objects with composite key (country, age, name).
Array sizes $n \in \{1\text{K}, 10\text{K}, 50\text{K}, 100\text{K}, 500\text{K}, 1\text{M}\}$.
Dirty counts $k \in \{1, 5, 10, 20, 50, 100, 200, 500, 1000, 2000, 5000, 10000, 20000\}$.
100 iterations per configuration with 95\% confidence intervals reported.

\subsection{Results}

Table~\ref{tab:rust-results} shows timing results for $n = 50,000$ elements.
DeltaSort consistently outperforms all alternatives up to approximately $k = 10,000$
(20\% of array size), achieving speedups of 7--17$\times$ over Native sort in the
sweet spot ($20 \leq k \leq 200$).

\begin{table}[t]
\centering
\caption{Execution time (\textmu s) for $n = 50,000$ elements. DS vs Best shows speedup
against the fastest alternative (Native, BI, or ESM).}
\label{tab:rust-results}
\begin{tabular}{r r r r r l}
\toprule
$k$ & Native & BI & ESM & DeltaSort & DS vs Best \\
\midrule
1     & 439 & 40 & 188 & \textbf{1} & 62$\times$ faster \\
5     & 749 & 217 & 293 & \textbf{27} & 8$\times$ faster \\
10    & 747 & 525 & 494 & \textbf{44} & 11$\times$ faster \\
20    & 962 & 782 & 655 & \textbf{49} & 13$\times$ faster \\
50    & 1,350 & 1,994 & 1,307 & \textbf{98} & 13$\times$ faster \\
100   & 1,867 & 4,818 & 2,673 & \textbf{109} & 17$\times$ faster \\
200   & 2,560 & 9,022 & 4,785 & \textbf{178} & 14$\times$ faster \\
500   & 3,703 & 22,112 & 11,655 & \textbf{782} & 5$\times$ faster \\
1,000  & 4,253 & 44,481 & 22,480 & \textbf{465} & 9$\times$ faster \\
2,000  & 4,158 & 87,827 & 43,627 & \textbf{837} & 5$\times$ faster \\
5,000  & 4,223 & 211,016 & 102,198 & \textbf{1,751} & 2.4$\times$ faster \\
10,000 & 4,539 & 377,981 & 176,373 & \textbf{3,326} & 1.4$\times$ faster \\
20,000 & \textbf{5,042} & 615,645 & 259,835 & 6,914 & 1.4$\times$ slower \\
\bottomrule
\end{tabular}
\end{table}

Figure~\ref{fig:crossover} visualizes the crossover between DeltaSort and Native sort
on a linear scale. The crossover occurs at $k \approx 13,700$ (27\% of array size),
clearly showing the region where DeltaSort provides substantial speedups.

\input{figures/crossover}

Figure~\ref{fig:all-algorithms} compares all four algorithms on a log-log scale,
revealing the orders-of-magnitude performance gap between DeltaSort and the baseline
algorithms (BI and ESM) across the full range of $k$ values.

\input{figures/all-algorithms}

\subsection{Crossover Analysis}

A key practical question is: at what delta size should one switch from DeltaSort to
Native sort? A binary search was conducted for the crossover point $k_c$ across array
sizes from 1K to 1M elements. Table~\ref{tab:crossover} summarizes the results.

\begin{table}[t]
\centering
\caption{Crossover point $k_c$ where Native sort becomes faster than DeltaSort.}
\label{tab:crossover}
\begin{tabular}{r r r}
\toprule
$n$ & $k_c$ & $k_c / n$ \\
\midrule
1,000 & 235 & 23.5\% \\
2,000 & 469 & 23.4\% \\
5,000 & 1,211 & 24.2\% \\
10,000 & 2,813 & 28.1\% \\
20,000 & 5,782 & 28.9\% \\
50,000 & 13,672 & 27.3\% \\
100,000 & 31,251 & 31.3\% \\
200,000 & 54,688 & 27.3\% \\
500,000 & 105,469 & 21.1\% \\
1,000,000 & 156,251 & 15.6\% \\
\bottomrule
\end{tabular}
\end{table}

The crossover ratio $k_c / n$ ranges from approximately 15\% to 31\%, with most values
falling in the 20--30\% range. This suggests a practical rule of thumb:

\begin{quote}
\emph{Use DeltaSort when fewer than $\sim$25\% of elements are dirty; otherwise use Native sort.}
\end{quote}

\subsection{JavaScript Implementation}

DeltaSort was also implemented in TypeScript running on Node.js v20 (V8 engine). While
the implementation passes all correctness tests, the performance results show higher
variance due to JIT compilation behavior, garbage collection pauses, and other
engine-level effects. The JavaScript benchmarks will be refined in a future revision
to provide more stable measurements. The Rust implementation provides the authoritative
performance characterization.

\subsection{Analysis}

\paragraph{DeltaSort vs.\ Binary Insertion.}
DeltaSort dramatically outperforms BI across all tested configurations, with speedups
ranging from 8$\times$ to over 100$\times$. This is because BI's $O(kn)$ extraction and
insertion costs dominate, while DeltaSort's segmentation enables coordinated processing
that avoids redundant movement.

\paragraph{DeltaSort vs.\ ESM.}
DeltaSort also outperforms ESM for all tested $k$ values up to 10,000. ESM's $O(n)$
merge pass becomes competitive only when $k$ is very large, and even then DeltaSort
remains faster in these measurements.

\paragraph{DeltaSort vs.\ Native Sort.}
The crossover with Native sort occurs at approximately 20--30\% dirty elements. Below
this threshold, DeltaSort's $O(k \log n)$ complexity and efficient stack-based processing
provide substantial speedups. Above this threshold, Native sort's highly optimized
$O(n \log n)$ implementation wins.

\paragraph{Algorithm Selection Guide.}
Based on the Rust benchmarks across array sizes from 1K to 1M elements:

\begin{center}
\begin{tabular}{ll}
\toprule
Condition & Recommendation \\
\midrule
$k \leq 5$ & Binary Insertion (skip Phase 1 overhead) \\
$5 < k < 0.25n$ & \textbf{DeltaSort} \\
$k \geq 0.25n$ & Native Sort \\
\bottomrule
\end{tabular}
\end{center}

\FloatBarrier
%==============================================================================
\section{Future Work}
\label{sec:future}
%==============================================================================

\paragraph{Formal Movement Bounds.}
A conjecture is that Phase 1's segmentation property reduces total element displacement
compared to uncoordinated binary insertion. A formal proof characterizing the expected
movement reduction---potentially in terms of the ``inversion distance'' between original
dirty values and their indices---would strengthen the theoretical foundation.

\paragraph{Cache-Aware Analysis.}
Formalize why bounded search ranges and localized segment processing help despite
unchanged asymptotic complexity.

\paragraph{Block Storage.}
Analyze DeltaSort for B-tree maintenance, where batched updates may reduce node splits.

\paragraph{JavaScript Implementation Refinement.}
The TypeScript/Node.js implementation shows higher variance due to JIT compilation,
garbage collection, and engine-level effects. Future work will characterize and minimize
these effects to provide stable JavaScript benchmarks.

\paragraph{Adaptive Hybrid.}
Runtime selection between DeltaSort and Native sort based on the 25\% crossover threshold,
potentially with dynamic adjustment based on observed performance.

%==============================================================================
\section{Conclusion}
\label{sec:conclusion}
%==============================================================================

This paper presented DeltaSort, a coordinated incremental repair algorithm for sorted arrays.
The key insight is that a pre-sorting phase creates \emph{directional segments}---contiguous
groups of dirty elements that all need to move in the same direction---which can then be
processed efficiently via stack-based coordination.

The main contributions are:

\begin{enumerate}
  \item \textbf{Segmentation via pre-sorting}: Establishing monotonicity among dirty values
        creates directional segments that enable coordinated processing.
  \item \textbf{Stack-based coordination}: LEFT segments are processed immediately with
        progressive search narrowing; RIGHT segments are deferred to a stack and processed
        in LIFO order, ensuring stable target positions.
  \item \textbf{Optimal comparisons}: $O(k \log n)$, matching the information-theoretic
        lower bound.
  \item \textbf{Substantial practical speedup}: 5--17$\times$ over native sort for delta
        sizes up to $\sim$25\% of array size, validated through Rust benchmarks across
        array sizes from 1K to 1M elements.
\end{enumerate}

The crossover point where native sort becomes faster occurs at approximately 25\% dirty
elements, providing a clear decision boundary for practitioners.

The broader lesson is that exploiting application-level knowledge (which indices changed)
enables coordination that blind algorithms cannot achieve. DeltaSort demonstrates that
careful coordination---segmentation plus stack-based processing---yields substantial
practical gains.

%==============================================================================
\bibliographystyle{plain}
\bibliography{refs}
\nocite{*}
\end{document}
