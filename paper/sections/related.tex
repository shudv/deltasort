%==============================================================================
\section{Related Work}
\label{sec:related}
%==============================================================================

\textbf{Adaptive sorting algorithms} exploit existing order in the input to improve performance on nearly sorted data. TimSort~\cite{timsort}, DriftSort~\cite{driftsort}, and natural mergesort~\cite{knuth1998art} dynamically identify monotonic runs and merge them efficiently, while a substantial body of work formalizes measures of presortedness and analyzes sorting complexity as a function of these measures rather than input size alone~\cite{mannila1985measures}. These approaches, however, assume no explicit knowledge of which values have been updated.

\textbf{Incremental view maintenance (IVM)} is a fundamental and deeply researched problem, where updates to input data are propagated to derived results using explicit delta representations~\cite{gupta1995maintenance, nikolic2014incremental, akidau2015dataflow, 7184878}. These techniques focus on maintaining query results, indexes, aggregates, and materialized views, and operate at the higher level of relational or dataflow operators rather than array-based sorting primitives. They treat deltas as first-class citizens and demonstrate the practical value of update-aware computation. However, they do not specifically address the lower level problem of incrementally sorting arrays, which is a much more specialized problem.

\textbf{Partial or selection sorting} is another problem of interest where sorted output is enumerated incrementally to avoid sorting everything upfront~\cite{paredes2006optimal}. This is a different model which addresses efficient selection rather than maintaining a fully sorted array under updates at all times.

\textbf{Dynamic data structures} offer a different trade-off. Self-balancing trees such as AVL trees~\cite{avl1962}, red--black trees~\cite{guibas1978dichromatic}, B-trees~\cite{bayer1972organization}, and skip lists~\cite{pugh1990skip} support efficient ordered updates with logarithmic cost, but they do not preserve the contiguous layout and its associated advantages. A related idea is ``library sort''~\cite{bender2004insertion}, which accelerates repeated insertions by maintaining gaps in the array; however, it uses $O(n)$ space and does not leverage explicit knowledge of update positions, addressing a different trade-off.

\textbf{In-place merging algorithms} focus on merging two sorted arrays without using any additional space~\cite{kronrod1969optimal,franceschini2003optimal}. While these are similar to DeltaSort as DeltaSort also performs merging without using linear auxiliary space, but the models and goals differ. In-place merging assumes both input arrays are fully sorted and prioritizes minimizing asymptotic space complexity, whereas DeltaSort deals with sorted arrays where some values have been updated and can be arbitrarily interspersed within the array, and the aim is to minimize data movement. Additionally, while in-place merging algorithms often have high constant-factor overheads and complex implementations~\cite{katajainen1996practical}, DeltaSort aims for practical efficiency with simpler design.

In contrast, this work focuses on a more specialized (but fundamental) problem: maintaining sorted order in contiguous arrays after a batch of in-place updates, under the assumption that the indices of updated values are explicitly available. This setting is not addressed by adaptive sorting algorithms, system-level incremental indexing techniques, selection-based partial sorting algorithms or in-place merging algorithms. \emph{DeltaSort} operates as a low-level sorting primitive that complements higher-level incremental systems while preserving the cache locality of array-based layouts. To our knowledge, \emph{no prior work has considered this exact sorting model}, and DeltaSort is the first algorithm to exploit explicit updated index information to achieve a non-trivial time-space trade-off between BIS and ESM.