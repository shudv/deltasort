\appendix

\section{DeltaSort Worst Case Movement}
\label{sec:appendix-worst-case}
DeltaSort can exhibit worst-case movement of $O(k n)$ under clustered monotic updates as shown in \figref{fig:worst-case}. In this case, all left-moving values must be shifted to the start of the array, and all right-moving values to the end of the array, resulting in maximal data movement of $O(kn)$.

\input{\figdir/worst-case}

\section{Full proof for expected number of segments}
\label{sec:appendix-movement-proof}
\begin{lemma}
  In Phase~1 of DeltaSort, the expected number of segments created is $O(\sqrt{k})$.
\end{lemma}
\label{lem:expected-segments}
\begin{proof}
After Phase~1, we have two sorted sequences of $k$ values each:
\begin{itemize}
  \item \textbf{Indices:} The $k$ updated positions $i_0 < i_1 < \cdots < i_{k-1}$, sorted.
  \item \textbf{Values:} The $k$ updated values $v_0 \le v_1 \le \cdots \le v_{k-1}$, sorted.
\end{itemize}
Phase~1 pairs these by rank: the $j$-th smallest value $v_j$ is placed at the $j$-th smallest index $i_j$. Since both indices and values are drawn uniformly at random, each sorted sequence forms an \emph{order statistic} of $k$ uniform samples.

Although values may be drawn from a different range than indices, what matters for determining direction is the value's \emph{target position}---where it belongs in the final sorted array. For a value $v_j$ in a sorted array of $n$ values spanning range $[0, M)$, its target position is approximately $t_j = v_j \cdot n / M$. Since $v_j$ is uniform over $[0, M)$, the target position $t_j$ is uniform over $[0, n)$---the same range as the indices. Thus, we can analyse both sequences as order statistics of uniform samples from the same range.

Define the \emph{gap} at rank $j$ as:
\[
g_j = t_j - i_j
\]
This measures how far ahead (positive) or behind (negative) the value's target position is relative to its current position. By \defref{def:direction}:
\begin{itemize}
  \item $g_j < 0$ implies the value is too small for its position, so direction is $L$.
  \item $g_j \ge 0$ implies the value is adequate or too large, so direction is $R$.
\end{itemize}

The gap evolves as we move through the ranks. Let $\Delta i_j = i_{j+1} - i_j$ and $\Delta t_j = t_{j+1} - t_j$ denote the spacings between consecutive order statistics. The change in gap is:
\[
g_{j+1} - g_j = \Delta t_j - \Delta i_j
\]
Since both $\Delta t_j$ and $\Delta i_j$ are spacings of independent uniform order statistics, they have the same expected value ($n/(k+1)$) and are independent of each other. Therefore, the increment $g_{j+1} - g_j$ has mean zero and behaves as a random step. This means the sequence $g_1, g_2, \ldots, g_k$ evolves as a \emph{symmetric random walk}.

By \defref{def:segment}, segment boundaries occur at $L \to R$ transitions---precisely when the gap $g_j$ crosses from negative to non-negative. This corresponds to an \emph{upcrossing} of zero in the random walk. A classical result from random walk theory~\cite{feller1968} states that a symmetric random walk of $k$ steps has expected number of zero crossings:
\[
\mathbb{E}[\text{zero crossings}] = \Theta(\sqrt{k})
\]
Since $L \to R$ transitions account for roughly half of all zero crossings, the expected number of segments is:
\[
\mathbb{E}[\text{segments}] = 1 + \mathbb{E}[L \to R \text{ transitions}] = O(\sqrt{k}) \qedhere
\]
\end{proof}

\section{Empirical Validation of Complexity Bounds}
\label{sec:appendix-complexity}

\figref{fig:complexity-analysis} presents empirical validation of DeltaSort's complexity bounds across multiple scales of $n$ and $k$. For movement complexity, we normalize the measured movement by $n\sqrt{k}$; for comparison complexity, we normalize by $k\log n$. If the theoretical bounds are tight, these normalised values should be approximately constant across all scales.

The results confirm both bounds: movement normalised by $n\sqrt{k}$ converges to approximately $0.4$ across four orders of magnitude of $n$, and comparisons normalised by $k\log n$ converge to approximately $2$. The overlap of lines for different $n$ values validates that the asymptotic analysis correctly captures the algorithm's behavior.

\input{\figdir/complexity-analysis}

\section{JavaScript/V8 Benchmarks}
\label{sec:appendix-js-deltasort}

\input{\figdir/js/performance}

\figref{fig:js-performance} shows execution time and crossover threshold data for JavaScript/V8. Several observations emerge:

\begin{enumerate}
  \item DeltaSort and BIS show almost identical performance profiles. This indicates that \emph{the underlying memory movement cost in V8 is not proportional to the number of moved values}. As a result, DeltaSort's core optimization---reducing physical data movement through segmentation---does not translate into wall-clock improvements. This could be because V8 has different kinds of array representations~\cite{v8elementskinds} that may not guarantee contiguous layouts.
  \item ESM is $\sim 40\%$ faster than FullSort (\texttt{Array.prototype.sort}) for $k$ up to $\approx 50\%$. This indicates that \textbf{even in managed runtimes, we can leverage information of updated indices to unlock better performance}.
\end{enumerate}

The key takeaway is that \emph{DeltaSort's performance benefits rely on a predictable movement cost model}, which holds in low-level, unmanaged environments (e.g., Rust) but not in managed runtimes such as V8. Hence in V8, a valid hybrid strategy can be: use BIS for $k \ll n$, ESM for $k \lesssim 40\%$, \texttt{Array.prototype.sort} for $k \gtrsim 40\%$.

\section{Raw Benchmark Data}
\label{sec:appendix-raw-data}

Table~\ref{tab:execution-time-Rust} presents the raw execution time measurements for Rust benchmarks. Each row shows the mean execution time and 95\% confidence interval as a percentage for a given value of $k$ (number of updated indices) with $n = 100{,}000$ elements.

\def\csvpath{rust/execution-time.csv}
\def\csvlabel{Rust}
\input{\figdir/execution-time-table}
