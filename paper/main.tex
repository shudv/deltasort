\documentclass[11pt]{article}

\usepackage{amsmath,amssymb}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{geometry}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{placeins}
\usepackage{float}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning,arrows.meta,patterns,calc,decorations.pathreplacing,pgfplots.fillbetween}

\geometry{margin=1in}

% Theorem environments
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{claim}[theorem]{Claim}

\title{DeltaSort: Incremental repair of sorted arrays with known updates}
\author{Shubham Dwivedi \\
\small Independent Researcher \\
\small \texttt{shubd3@gmail.com}
}
\date{January 2026}

\begin{document}
\maketitle

\begin{abstract}
Reading sorted data is a fundamental requirement in almost all data processing systems. When dealing with large sorted datasets that require great read performance, sorting-on-read is not feasible. A standard approach is to have a derived sorted read-replica that is updated with the latest snapshot asynchronously whenever the system-of-record gets updated. For updating read-replicas, most production systems resort to either full re-sorting or Binary-Insertion-Sort or Extract-Sort-Merge. In this paper, we present why the traditional approaches are sub-optimal and how we can do better. This paper also formulates an alternative model in which sorting routine is explicitly informed of the updated indices since the previous sort. Under this model, we present \emph{DeltaSort}, an efficient incremental repair algorithm for arrays. We present experimental evidence that shows that \emph{DeltaSort} outperforms existing approaches for a wide range of update batch sizes. These results suggest that tighter integration between update pipelines and sorting routines can yield significant performance gains in real incremental-sorting workloads. We also explore limitations of this approach and show that there is a clear crossover point depending on array size where full re-sorting becomes preferable. 
\end{abstract}

%==============================================================================
\section{Introduction}
%==============================================================================

Sorting is among the most optimized primitives in modern systems backed by decades of deep research. Standard library implementations—TimSort~\cite{timsort}, Introsort~\cite{musser1997introspective}, and PDQSort~\cite{peters2021pdqsort} deliver excellent performance for general inputs by exploiting partial order, cache locality, and adaptive strategies. However, these algorithms operate under a \emph{blind} model: they rediscover structure dynamically rather than being explicitly informed about which elements have changed since the previous sort.

In many practical systems, this assumption is unnecessarily pessimistic. Sorted arrays are often maintained incrementally in read-heavy workloads where updates affect only a subset of elements and the indices of those updates can be easily tracked if needed. Nevertheless, this information is typically not tracked or utilized, and systems fall back to blind re-sorting or sub-optimal repair approaches using Binary-Insertion-Sort or Extract-Sort-Merge. As an example, consider a web application that shows a real-time game leaderboard. Being able to efficiently update the sorted view on updates directly impacts the user experience. The traditional approaches don't serve us very well - full re-sort can block the main UI thread for long durations, while Binary-Insertion-Sort can be slow for large number of updates. To address these limitations, this paper makes the following contributions:

\begin{enumerate}
  \item \textbf{Update-aware sorting model:}
  We formulate an incremental sorting model in which the sorting routine is explicitly informed of the updated indices since the previous sort. Under this model, Binary-Insertion-Sort and Extract-Sort-Merge are standard baseline algorithms because they help maintain sorted order by processing incremental updates.

  \item \textbf{DeltaSort algorithm:}
  We present \emph{DeltaSort}, an incremental repair algorithm for sorted arrays designed for the update-aware model. DeltaSort batches updates and repairs them jointly, reducing redundant comparison work and overlapping element movement by exploiting update locality and batching. DeltaSort achieves multi-fold speed ups over traditional approaches and also beats Native-Sort for a wide range of update batch sizes.
\end{enumerate}

%==============================================================================
\section{Related Work}
\label{sec:related}
%==============================================================================

Adaptive sorting algorithms exploit existing order in the input to improve performance on nearly sorted data. TimSort~\cite{timsort} and natural merge sort~\cite{knuth1998art} identify monotonic runs dynamically and merge them efficiently, yielding improved performance when such structure is present. A substantial body of work formalizes measures of presortedness and studies sorting algorithms whose complexity depends on these measures rather than input size alone~\cite{mannila1985measures}. These approaches, however, must discover structure through full-array scans and do not assume explicit knowledge of which elements were modified.

Dynamic data structures offer a different trade-off. Self-balancing trees such as AVL trees~\cite{avl1962}, red--black trees~\cite{guibas1978dichromatic}, B-trees~\cite{bayer1970organization}, and skip lists~\cite{pugh1990skip} support efficient ordered updates with logarithmic cost, but sacrifice contiguous memory layout and cache locality. Library sort~\cite{bender2006insertion} reduces insertion cost by maintaining gaps in the array, but addresses online insertion and incurs additional space overhead.

In contrast, this work focuses on maintaining sorted order in arrays under batched updates where the indices of updated elements since the last sort are explicitly available. This update-aware model enables efficient repair without auxiliary data structures, and distinguishes \emph{DeltaSort} from prior adaptive and incremental approaches.

%==============================================================================
\section{Problem Model}
\label{sec:model}
%==============================================================================

\begin{definition}[Update-Aware Sorting]
\label{def:problem}
Let $A[0..n-1]$ be an array sorted according to a strict weak ordering defined by a comparator $\texttt{cmp}$. Let $D = \langle d_1, d_2, \dots, d_k \rangle$ be a sequence of indices such that \[ 0 \le d_1 < d_2 < \dots < d_k \le n-1, \] and the values at these indices may have been arbitrarily modified, while values at all other indices remain unchanged. The \emph{update-aware sorting problem} is to restore $A$ to a state that is sorted with respect to $\texttt{cmp}$, given explicit knowledge of the set $D$.
\end{definition}

%==============================================================================
\section{DeltaSort Algorithm}
\label{sec:algorithm}
%==============================================================================

\subsection{Overview}

\begin{definition}[Violation]
\label{def:violation}
For an updated index $i$, we classify its \emph{violation} based on local order:
\begin{itemize}
  \item \textbf{LEFT (L)}: Element \textbf{must} move left — $\texttt{cmp}(A[i-1], A[i]) > 0$ (for $i > 0$).
  \item \textbf{RIGHT (R)}: Element \textbf{may} move right or stay stable.
\end{itemize}
\end{definition}

Every updated index is either a LEFT or RIGHT violation. Violation is only applicable to updated indices and has no meaning for clean indices.

\begin{definition}[Segment]
\label{def:segment}
A \emph{segment} is a \textbf{maximal subarray} of $D$ whose elements follow the pattern $(\mathrm{R})^*(\mathrm{L}^+ \mid \epsilon)$: zero or more RIGHT violations followed by one or more LEFT violations, or the empty suffix if the sequence ends.
\end{definition}

\input{figures/segment-diagram}

DeltaSort operates in two phases:

\begin{enumerate}
  \item \textbf{Phase 1 (Preparation):} Extract updated values sort them, write back to updated indices in index order. This establishes directional segments in the array which are disjoint and can be repaired independently.
  \item \textbf{Phase 2 (Repair):} Repair each segment left-to-right, deferring RIGHT indices to a stack until the first LEFT index is encountered. Flush and repair the RIGHT indices in stack in LIFO order. Then repair all LEFT indices left-to-right.
\end{enumerate}

\subsection{Key Insight: Directional segmentation enables localized repair}
\label{sec:insight}

The key insight behind DeltaSort is that pre-sorting updated values induces a \emph{directional segmentation} of updates. After Phase~1, updated indices partition into disjoint segments of the form $(\mathrm{R})^*(\mathrm{L}^+ \mid \epsilon)$.

\begin{lemma}[Movement Confinement]
\label{lem:confinement}
Element movement during repair phase is bounded within each segment: no element crosses a segment boundary.
\end{lemma}

\begin{proof}
Let $S$ be a segment with RIGHT indices $R_1, \ldots, R_m$ followed by LEFT indices
$L_1, \ldots, L_p$. After Phase~1, dirty values are monotonically ordered by index,
so $A[R_1] < \cdots < A[R_m] < A[L_1] < \cdots < A[L_p]$.

\begin{enumerate}
  \item \emph{RIGHT elements cannot pass the leftmost LEFT.}
        Since $A[R_i] < A[L_1]$ for all $i$.

  \item \emph{LEFT elements cannot pass the rightmost RIGHT.}
        Since $A[R_m] < A[L_j]$ for all $j$.
\end{enumerate}

Since no element exits its segment, segments can be repaired independently.
\end{proof}

\subsection{Detailed Algorithm}

\input{figures/algorithm-detailed}

\subsection{Correctness Proof}

\begin{lemma}[Initial Segment Boundary Order]
\label{lem:initial-boundary}
For any two consecutive segments induced after Phase~1, let $i$ be the index of the last updated element in first segment and $j$ be the index of the first updated element in the second segment. Then $\texttt{cmp}(A[i], A[j]) \le 0$ holds immediately after Phase~1.
\end{lemma}

\begin{proof}
By construction, Phase~1 extracts all updated values, sorts them according to $\texttt{cmp}$, and writes them back to updated indices in increasing index order. Thus, for any two updated indices $d_p < d_q$, we have $\texttt{cmp}(A[d_p], A[d_q]) \le 0$ after Phase~1. Since $i < j$ are consecutive updated indices belonging to adjacent segments, the claim follows directly.
\end{proof}

\begin{lemma}[Fixing Does Not Introduce Violations]
\label{lem:fix-invariant}
When a LEFT or RIGHT element is moved to its correct position via binary search, no new violations are introduced.
\end{lemma}

\begin{proof}
\emph{LEFT fix}: Binary search finds position $t < i$ where the element belongs. Elements in $A[t..i-1]$ shift right by one. The shift preserves relative order. By binary search, the moved element satisfies $A[t-1] \leq A[t] < A[t+1]$.

\emph{RIGHT fix}: Binary search finds position $t > s$ where the element belongs. Elements in $A[s+1..t]$ shift left by one. The shift preserves relative order. By binary search, the moved element satisfies $A[t-1] < A[t] \leq A[t+1]$.

In both cases, no new violations are introduced.
\end{proof}

\begin{theorem}[Correctness]
\label{thm:correctness}
DeltaSort produces a correctly sorted array.
\end{theorem}

\begin{proof}
Phase~2 processes each dirty index exactly once, fixing its violation if any. By Lemma~\ref{lem:fix-invariant}, each fix resolves a violation without introducing new ones. After all dirty indices are processed, each segment is internally sorted. By Lemma~\ref{lem:initial-boundary} and Lemma~\ref{lem:confinement}, segment boundaries start with correct order and maintain correct order throughout. Since each segment is internally sorted and boundaries preserve global order, the entire array is sorted.
\end{proof}

\subsection{Algorithm Analysis}

\begin{theorem}[Time Complexity]
\label{thm:time}
DeltaSort runs in $O(k \log k + k \log n + M)$ time, where $M$ is total movement.
\end{theorem}

\begin{proof}
We analyze the two phases separately.

\paragraph{Phase 1.}
Sorting the $k$ dirty indices and their corresponding values costs $O(k \log k)$ time.
Writing the sorted values back requires $O(k)$ time.

\paragraph{Phase 2.}
Each dirty index is processed once. Direction checks cost $O(1)$ per index, and each repair
performs a binary search over a sorted region in $O(\log n)$ time. Thus Phase~2 requires $O(k \log n)$ time. Let $M$ denote the total number of elements moved during repair. The movement cost is $O(M)$.

\paragraph{Total.}
The overall complexity is $O(k \log n)$ time and $O(M)$ data movement.
\end{proof}

\begin{theorem}[Space Complexity]
\label{thm:space}
DeltaSort uses $O(k)$ auxiliary space.
\end{theorem}

\begin{proof}
Phase~1 stores $k$ dirty indices and $k$ dirty values. Phase~2 maintains a pending stack of at most $k$ indices. No $O(n)$ auxiliary structures are required.
\end{proof}

\begin{remark}[Movement Efficiency]
While worst-case movement is $O(kn)$, the segmentation created by Phase 1 tends to reduce movement in practice in the average case. Empirical results in \S\ref{sec:experiments} demonstrate substantial speedups, validating that movement is typically much less than the worst case. We will analyze average-case movement more rigorously in future work.
\end{remark}

Here we compare algorithm complexity with other standard baseline approaches which we use in experiments
to compare DeltaSort's performance:
\begin{itemize}
  \item \textbf{NativeSort (NS)}: Re-sort the array using the natively available sort function. $O(n \log n)$ comparisons and movements.
  \item \textbf{Binary-Insertion-Sort (BIS)}: Extract dirty values, then for each: binary search for correct position, reinsert. $O(k \log n)$ comparisons, $O(kn)$ worst-case movement. Searches the full array range for each insertion.
  \item \textbf{Extract-Sort-Merge (ESM)}: Extract dirty values, sort them, merge with clean elements. $O(k \log k + n)$ comparisons, $O(n)$ movement. Requires $O(n)$ auxiliary space.
\end{itemize}

\begin{table}[h]
\centering
\caption{Algorithm complexity comparison}
\label{tab:complexity}
\begin{tabular}{l c c c}
\toprule
Algorithm & Comparisons & Movement & Space \\
\midrule
NativeSort & $O(n \log n)$ & $O(n \log n)$ & $O(n)$ \\
Binary-Insertion-Sort & $O(k \log n)$ & $O(kn)$ & $O(1)$ \\
Extract-Sort-Merge & $O(k \log k + n)$ & $O(n)$ & $O(n)$ \\
\textbf{DeltaSort} & $O(k \log n)$ & $O(kn)$* & $O(k)$ \\
\bottomrule
\end{tabular}

\vspace{0.3em}
{\small *Worst case; average case is closer to $O(n)$.}
\end{table}

%==============================================================================
\section{Experimental Evaluation}
\label{sec:experiments}
%==============================================================================

All experiments are run using a Rust implementation of DeltaSort~\cite{deltasort-repo} on a synthetic dataset of user objects with composite keys (country, age, name) on a M3 Pro Macbook Pro with 18GB RAM. DeltaSort is compared against three baselines: NativeSort (Rust's \texttt{sort\_by}), Binary Insertion Sort (BIS), and Extract-Sort-Merge (ESM).

\subsection{Correctness}
Correctness is formally-proven as Theorem~\ref{thm:correctness} and also verified by an extensive set of randomized unit tests~\cite{deltasort-repo} across various scales and update sizes. The test routine generates a sorted base array of size $N$, applies $k$ random updates at random indices, runs DeltaSort, and asserts that the final array is sorted and contains all original elements with updated values. All tests pass successfully.

\subsection{Execution Time}

The graph below shows execution time (in microseconds) for $n = 50,000$ elements as a function of dirty count $k$. DeltaSort consistently outperforms all alternatives up to approximately $k = 15K$ (crossover point), achieving significant speedups of 6--20$\times$ over Native sort in the intermediate range. Also note the orders-of-magnitude gap between DeltaSort and the baseline incremental algorithms (BIS and ESM) across the full range of $k$ values.

\input{figures/all-algorithms}

\subsection{Comparator Invocation Count}

The following chart shows the number of comparator invocations for each algorithm. DeltaSort and BIS both achieve $O(k \log n)$ comparisons, substantially fewer than Native Sort's $O(n \log n)$ and ESM's $O(k \log k + n)$. The comparison counts for DeltaSort are 10--40\% higher than BIS, indicating that DeltaSort's performance advantage comes from reduced data movement (Lemma ~\ref{lem:confinement}).

\input{figures/comparator-count}

\subsection{Crossover Threshold Analysis}

A key practical question is: at what delta size should one switch from DeltaSort to Native sort? A binary search was conducted for the crossover point $k_c$ across array sizes from 1K to 10M elements.

Figure~\ref{fig:crossover-ratio} visualizes how the crossover ratio $k_c / n$ varies with array size. The ratio peaks around 31\% for medium-sized arrays ($n \approx 50$K) and declines for very large arrays, suggesting that DeltaSort's advantage  narrows as arrays grow very large. More study is needed to understand this trend fully, because intuitively the trend should holdup much better than the emperical data seems to suggest.

\input{figures/crossover-ratio}

The key takeaway is that DeltaSort is beneficial for \textbf{large} ranges of update sizes. So DeltaSort remains a viable choice even when delta sizes are large. The exact threshold depends on the scenario specifics (array sizes, data types, comparator cost, etc.) but the results suggest that a \textbf{25\% dirty fraction} is a reasonable rule of thumb for the upper limit of delta size before which DeltaSort remains advantageous.

\subsection{Performance in managed execution environments}

DeltaSort was also implemented in JavaScript~\cite{deltasort-repo} and benchmarked on the V8 engine to evaluate behavior in managed runtimes. Initial results indicate that DeltaSort outperforms NativeSort for some workloads, but the gains are more modest and the crossover point occurs at smaller update sizes. This behavior appears to be driven primarily by the performance characteristics of the native JavaScript sort, which is highly optimized and handles nearly sorted inputs exceptionally well. As a result, the relative advantage of coordinated repair is reduced in this environment compared to Rust implementation. The JavaScript benchmarks are still being refined and will be reported in a later revision. Until then, the Rust implementation provides the primary and authoritative performance characterization.

%==============================================================================
\section{Future Work}
\label{sec:future}
%==============================================================================

This work suggests several directions for future investigation:

\begin{itemize}
  \item \textbf{Average-case movement bounds:}
  Given that the emperical results suggest multi-fold speedsups over NativeSort for Rust implementation, it would be valuable to derive tighter average-case bounds on data movement $M$ in Theorem~\ref{thm:time}.

  \item \textbf{Structured workload analysis:}
  The current evaluation relies on randomized updates, whereas many real
  workloads exhibit additional structure, such as gradual value changes in leaderboards or localized updates in interactive list views. Studying such patterns may reveal regimes where DeltaSort’s advantages are amplified or diminished.

  \item \textbf{Study managed environments:}
  Performance variance in managed environments warrants deeper investigation. Understanding the impact of factors such as garbage collection, and JIT compilation will help in explaining the observed performance characteristics.

  \item \textbf{Block-structured storage:}
  Although this work focuses on in-memory arrays, the update-aware model naturally extends to block-structured storage as well. Exploring how DeltaSort-style segmentation interacts with page- or block-based layouts may clarify its applicability to database and external-memory settings.
\end{itemize}

%==============================================================================
\section{Conclusion}
\label{sec:conclusion}
%==============================================================================

This paper introduced \emph{DeltaSort}, an incremental repair algorithm for maintaining sorted arrays. The central insight is that pre-sorting updated values
induces \emph{directional segmentation}: dirty elements naturally partition into segments that can be repaired independently.

DeltaSort leverages this segmentation through stack-based processing. LEFT-moving elements are repaired immediately with progressively narrowed search ranges, while RIGHT-moving elements are deferred and processed in reverse order to ensure stable target positions. This segmentation avoids redundant comparisons and overlapping element movement that arise from repeated binary insertion.

An experimental evaluation in Rust demonstrates that DeltaSort consistently outperforms both blind native sorting and repeated binary insertion across a wide range of array sizes and update volumes. In practice, DeltaSort remains advantageous until approximately 25\% of the array is updated, providing a clear and actionable decision boundary.

More broadly, this work highlights the value of integrating application-level update information into core algorithms. When sorting routines are informed of which elements changed, batching and segmentation becomes possible, enabling performance improvements that blind algorithms cannot realize. DeltaSort illustrates how modest structural insight—segmentation combined with disciplined processing order—can yield substantial practical gains.

%==============================================================================
\bibliographystyle{plain}
\bibliography{refs}
\end{document}
