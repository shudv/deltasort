\documentclass[11pt]{article}

\usepackage{amsmath,amssymb}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{geometry}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{pgfplotstable}
\usepackage{placeins}
\usepackage{float}
\pgfplotsset{compat=1.18}

% Path to benchmark data (different for article vs SEA version)
\newcommand{\rootdir}{.}

% Load benchmark metadata from CSV
\pgfplotstableread[col sep=comma]{\rootdir/figures/rust/metadata.csv}\rustmetadata
\pgfplotstablegetelem{0}{value}\of\rustmetadata \let\rustbenchmarkdate\pgfplotsretval
\pgfplotstablegetelem{1}{value}\of\rustmetadata \let\rustbenchmarkmachine\pgfplotsretval
\pgfplotstablegetelem{2}{value}\of\rustmetadata \let\rustbenchmarkn\pgfplotsretval
\pgfplotstablegetelem{3}{value}\of\rustmetadata \let\rustbenchmarkiterations\pgfplotsretval

% Command to add benchmark note to figures
\newcommand{\rustbenchmarknote}{\par\vspace{2pt}\noindent{\scriptsize\textit{Benchmarked on \rustbenchmarkmachine\ (\rustbenchmarkdate), $n=\rustbenchmarkn$, \rustbenchmarkiterations\ iterations.}}}
\usetikzlibrary{positioning,arrows.meta,patterns,calc,decorations.pathreplacing,pgfplots.fillbetween}

\geometry{margin=1in}

% Theorem environments
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{claim}[theorem]{Claim}

\title{DeltaSort: Incremental repair of sorted arrays with known updates}
\author{Shubham Dwivedi \\
\small Independent Researcher \\
\small \texttt{shubd3@gmail.com}
}
\date{2026}

\begin{document}
\maketitle

\begin{abstract}
Reading sorted values or records is a fundamental operation in many systems. When records need to be read in a particular order, sorting-on-read incurs repeated $O(n \log n)$ cost which can become a bottleneck in read-heavy systems. A standard solution is to have a derived sorted read-replica that is updated with the latest snapshot asynchronously whenever the system-of-record gets updated. For updating read-replicas, existing approaches rely on either full re-sorting or incremental techniques such as Binary-Insertion-Sort or Extract–Sort–Merge, each providing different time-space tradeoffs. In this paper, we study incremental sorting under an alternative model where the sorting routine is explicitly informed of the indices of updated values since the previous sort. Under this model, we present \emph{DeltaSort}, a new algorithm technique for incremental sorting that offers distinct time-space trade-off. We present theoretical analysis and experimental evidence that show that \emph{DeltaSort} outperforms existing approaches for practically relevant regimes, while exhibiting clear crossover points beyond which other techniques become preferable.
\end{abstract}

%==============================================================================
\section{Introduction}
%==============================================================================

Sorting is among the most heavily optimized primitives in modern systems, backed by decades of deep research. Standard library implementations---TimSort~\cite{timsort}, Introsort~\cite{musser1997introspective}, and PDQSort~\cite{peters2021pdqsort} deliver excellent performance for general inputs by exploiting partial order, cache locality, and adaptive strategies. However, these algorithms operate under a \emph{blind} model: they discover structure dynamically rather than being explicitly informed about which values have changed since the previous sort.

In many practical systems, this assumption is unnecessarily pessimistic. Whenever values in an array are updated, their indices can be easily tracked by using extra space proportional to the number of updates. This is especially true in modern streaming and stateful architectures where updates are processed incrementally and stateful operators maintain metadata about updated records.

As an example, consider an analytics service that materializes and caches query results sorted by a particular field to serve repeat requests efficiently. The underlying data may be distributed across multiple clusters which may see multiple updates, but only a small fraction of those might be relevant for a particular caches query result. A common approach is to invalidate the cached results, which simplifies implementation but forces a full computation the next time, even when updates are sparse.

This pattern also arises in rich client applications. Consider a task management application that shows a large list of tasks sorted by priority or due date. Updates typically affect only a small subset of tasks at a time, such as when user completes a bunch of tasks. In most implementations, these edits trigger a full resort of the list even though the application already knows exactly which tasks were modified. This indicates that there is need for lower-level primitives that treat deltas as a first-class citizen.

Once the indices of the updates are known, a natural question arises: how do we use this extra information and repair the array more efficiently as compared to a full re-sort? Existing approaches (Table~\ref{tab:incremental-sorting-algorithms}) already exploit this extra information, but they force a choice between competing extremes-
\begin{enumerate}
  \item \textbf{Binary-Insertion-Sort (BIS):} For each updated index, remove the value and re-insert it at the correct position using binary search to find the insertion point. This approach uses $O(1)$ extra space, but $O(k n)$ data movement for $k$ updates in an array of size $n$, making it suitable only for very small update batches.
  \item \textbf{Extract–Sort–Merge (ESM):} Extract all updated values into a separate array, sort them using an efficient $O(k \log k)$ algorithm, and then merge them back into the original array. This approach uses $O(n + k \log k)$ time but $O(n)$ extra space even for small $k$, making it suitable only for larger update batches.
\end{enumerate}

This naturally raises the question: are there other intermediate strategies between these two extremes? In this paper, we present such a strategy. Specifically, this paper makes following contributions:

\begin{enumerate}
  \item \textbf{Update-aware sorting model:}
  We formulate a sorting model in which the sorting routine is explicitly informed of the updated indices since the previous sort. Under this model, Binary-Insertion-Sort and Extract-Sort-Merge serve as the baseline algorithms because they assume knowledge of what has been updated. 

  \item \textbf{DeltaSort algorithm:}
  We present \emph{DeltaSort}, an update-aware sorting algorithm, which offers a different set of trade-offs (Table~\ref{tab:incremental-sorting-algorithms}) as compared to the existing approaches. It outperforms Binary-Insertion-Sort in execution time for small update batches ($k \llless n$) and provides a more space-efficient alternative to Extract-Sort-Merge for larger update batches ($k \approx O(n)$).

  \input{figures/incremental-sorting-algorithms}

\end{enumerate}

%==============================================================================
\section{Related Work}
\label{sec:related}
%==============================================================================

Adaptive sorting algorithms exploit existing order in the input to improve performance on nearly sorted data. TimSort~\cite{timsort} and natural merge sort~\cite{knuth1998art} dynamically identify monotonic runs and merge them efficiently, while a substantial body of work formalizes measures of presortedness and analyzes sorting complexity as a function of these measures rather than input size alone~\cite{mannila1985measures}. These approaches, however, assume no explicit knowledge of which values have changed.

A separate line of work studies incremental computation and view maintenance in database and streaming systems, where changes to input data are propagated to derived results using explicit delta representations~\cite{gupta1995maintenance, nikolic2014incremental, akidau2015dataflow}. These techniques focus on maintaining query results, aggregates, and materialized views, and operate at the level of relational or dataflow operators rather than array-based sorting primitives. They treat deltas as first-class citizen and demonstrate the practical value of update-aware computation. Howevery, they do not specifically address the problem of repairing sorted arrays, which is much more specialized lower-level problem.

Dynamic data structures offer a different trade-off. Self-balancing trees such as AVL trees~\cite{avl1962}, red--black trees~\cite{guibas1978dichromatic}, B-trees~\cite{bayer1972organization}, and skip lists~\cite{pugh1990skip} support efficient ordered updates with logarithmic cost, but abandon contiguous array layout and its associated advantages.

In contrast, this work considers maintaining sorted order in arrays where the indices of updated values since the previous sort are explicitly available. Unlike adaptive sorting algorithms, which need to discover the presortedness, \emph{DeltaSort} already knows which values have changed and hence can leverage this information to achieve better performance. Unlike incremental view maintenance techniques, which focus on higher-level query results, \emph{DeltaSort} is a low-level sorting primitive for arrays, intended to complement higher-level incremental systems rather than replace them. Unlike dynamic data structures, which sacrifice array layout for update efficiency, \emph{DeltaSort} maintains the contiguous array layout.

%==============================================================================
\section{Problem Model}
\label{sec:model}
%==============================================================================

\begin{definition}[Update-Aware Sorting]
\label{def:problem}
Let $A[0..n-1]$ be an array of size $n$ sorted according to a strict weak ordering defined by a comparator $\texttt{cmp}$. Let $U = \langle u_0, u_1, \dots, u_{k-1} \rangle$ be a sequence of $k$ indices such that \[ 0 \le u_0 < u_1 < \dots < u_{k-1} \le n-1, \] and the values at these indices may have been arbitrarily updated, while values at all other indices remain unchanged. The \emph{update-aware sorting problem} is to restore $A$ to a state that is sorted with respect to $\texttt{cmp}$, given explicit knowledge of the set $U$.
\end{definition}

\begin{remark}
The above model can be easily extended to account for additions and deletions from the array-
\begin{itemize}
  \item \textbf{Addition:} For an existing array of size $n$, append new value to end of the array and add $n$ (the new index) to $U$. 
  \item \textbf{Deletion:} To delete value at index $i$, set its value to a sentinel $\infty$ that is greater than all other values according to $\texttt{cmp}$ and add it to $U$. Once sorted, the array size can be reduced by one removing the last element.
\end{itemize}
\end{remark}

%==============================================================================
\section{DeltaSort Algorithm}
\label{sec:algorithm}
%==============================================================================

\subsection{Overview}

Before we get to how the algorithm works, let's establish useful terminology-

\begin{definition}[Direction]
\label{def:direction}
For an updated index $i$, we define its \emph{direction} based on local order:
\begin{itemize}
  \item \textbf{LEFT ($L$)}: Value \textbf{must} move left---$\texttt{cmp}(A[i-1], A[i]) > 0$ (for $i > 0$).
  \item \textbf{RIGHT ($R$)}: Value \textbf{may} move right and \textbf{cannot} move left.
\end{itemize}
\end{definition}

\begin{remark}
The definition of direction is deliberately asymmetric: L requires a definite violation ($A[i-1] > A[i]$), while R encompasses both violations and non-violations. One might consider a three-way classification with a separate S (stable) category for values already in their correct positions. However, this complicates the setup:
\begin{enumerate}
  \item S values need to be processed identically to R values (both may stay or move right).
  \item Fixing an R value can shift elements, potentially converting a previously S neighbor into an R violation. Example: consider array $[20, 30, 10]$ where indexes $0$ and $1$ were updated. Index $0 (20)$ is initially S because it is correctly places w.r.t index $1$ $(30)$, but once index $1$ is fixed, index $0$ becomes an R violation since $20 > 10$. 
\end{enumerate}
The two-way classification avoids this issue by maintaining a useful invariant (Lemma~\ref{lem:fix-invariant}): \emph{the direction of an updated index never changes until it is fixed}. This simplifies the correctness argument and implementation.
\end{remark}

\begin{definition}[Segment]
\label{def:segment}
A \emph{segment} is a pair of indices $(i, j)$ with $i < j$ satisfying:
\begin{enumerate}
  \item Either $i = 0$, or $i \in U$ with direction R.
  \item Either $j = n - 1$, or $j \in U$ with direction L.
  \item There do not exist updated indices $p, q \in U$ with $i < p < q < j$ such that $p$ has direction L and $q$ has direction R.
\end{enumerate}
  \item There do not exist updated indices $p < $i and $q > $j such that $(p, q)$ is also a segment.
More intuitively, a segment is a contiguous portion of the array where all R-direction updates precede all L-direction updates. Condition~(3) ensures that no L appears before an R within the segment. Condition~(4) ensures segments are maximal and non-overlapping. Conditions~(1) and~(2) handle boundary cases: a leading segment containing only L's is bounded by the array start, and a trailing segment containing only R's is bounded by the array end. Figure~\ref{fig:segment-structure} illustrates this structure.
\end{definition}

\input{figures/segment-structure}

Armed with these definitions, we can see how DeltaSort works. It operates in two phases:

\begin{enumerate}
  \item \textbf{Phase 1 (Segment):} Extract updated values, sort them, and write back to updated indices in index order. This establishes segments (Definition~\ref{def:segment}) in the array that are disjoint and can be \emph{repaired independently}.
  \item \textbf{Phase 2 (Repair):} Repair each segment left-to-right, deferring R indices to a stack until the first L index is encountered. When a L is encountered, first flush and repair all pending Rs in LIFO order, then repair the L. Continue left-to-right.
\end{enumerate}

Figure~\ref{fig:delta-sort-example} illustrates the full DeltaSort process on a small example.

\input{figures/delta-sort-example}

\subsection{Key Insight: \emph{Segmentation enables localized repair}}
\label{sec:insight}

The key insight behind DeltaSort is that pre-sorting updated values induces a \emph{segmentation} of updates. After Phase~1, updated indices partition into disjoint segments.

\begin{lemma}[Movement Confinement]
\label{lem:confinement}
Value movement during the repair phase is bounded within each segment: no value needs to cross a segment boundary.
\end{lemma}

\begin{proof}
Let $S$ be a segment with R indices $R_0, \ldots, R_{r-1}$ followed by L indices $L_0, \ldots, L_{l-1}$ (where $r \ge 0$ and $l \ge 0$, with $r + l \ge 1$). After Phase~1, updated values are monotonically ordered by index, so $A[R_0] < \cdots < A[R_{r-1}] < A[L_0] < \cdots < A[L_{l-1}]$.

\begin{enumerate}
  \item R values move rightward but cannot pass the first L value $L_0$ (if it exists) or the segment boundary, since $A[R_i] < A[L_0]$ for all $i$.

  \item L values move leftward but cannot pass the last R value $R_{r-1}$ (if it exists) or the segment boundary, since $A[R_{r-1}] < A[L_j]$ for all $j$.
\end{enumerate}

Since no value exits its segment, each segment can be repaired independently. The more segments we have after Phase~1, more localized fixes are possible. Theorem~\ref{thm:movement-bound} establishes an asymptotic bound on number of segments.
\end{proof}

\begin{remark}
  Note that segmentation also opens up opportunities for parallelization since segments can be repaired independently. This has practical implications for improving performance on multi-core systems or distributed environments. To avoid scope creep, we leave parallelization implications for future work.
\end{remark}

\subsection{Pseudocode}

\input{figures/pseudocode}

\subsection{Correctness Proof}

\begin{lemma}[Violation Fix Invariant]
\label{lem:fix-invariant}
Each fix operation during Phase~2 resolves an order violation without introducing new ones.
\end{lemma}

\begin{proof}
We fix each violation using binary search. For binary search to find the correct insertion point, the search range must contain no violations.
\begin{itemize}
    \item \emph{L fix at index $i$}: The search range $[leftBound, i-1]$ contains no L violations because Ls are processed left-to-right, and no R violations because all pending Rs are flushed before any L is fixed.
    \item \emph{R fix at index $i$}: The search range $[i+1, rightBound]$ contains no R violations because Rs are processed in LIFO order with $rightBound$ narrowing after each fix, and no L violations because $rightBound$ never extends past the first L in the segment.
\end{itemize}
\end{proof}

\begin{theorem}[Correctness]
\label{thm:correctness}
DeltaSort produces a correctly sorted array.
\end{theorem}

\begin{proof}
The only violations in the array after Phase~1 are at updated indices. Phase~2 processes each updated index exactly once. By Lemma~\ref{lem:fix-invariant}, each fix resolves a violation without introducing new ones. After all fixes, no violations remain, so the array is sorted.
\end{proof}

\subsection{Complexity Analysis}
\label{sec:complexity}

We analyze the expected total data movement incurred during Phase~2 of DeltaSort under a bounded-range update model. In this model, updated values are drawn independently and uniformly from a fixed value range. This choice ensures that updates do not introduce additional structure beyond what is captured by the directional segmentation created in Phase~1.

\begin{remark}[Choice of Update Model]
\label{rem:update-model}
Alternative models may also be considered. For example, under an unbounded-range model where updated values are drawn from an unrestricted domain, or under a perturbation model where updates introduce small random deviations from existing values, DeltaSort exhibits equal or strictly better expected movement behavior. We focus on the bounded-range model as it is conservative and avoids assuming favorable update structure. A more detailed treatment of alternative models is left to future work.
\end{remark}

\begin{theorem}[Expected Linear Movement]
\label{thm:movement-bound}
Under the bounded-range update model, for an array of size $n$ with $k$ updated indices, the expected total data movement incurred during DeltaSort’s repair phase is $O(n)$, independent of $k$.
\end{theorem}

\begin{proof}
Consider the sequence of directions induced by the $k$ updated indices after Phase~1. Let the updated indices be ordered by increasing index, and let position $i \in \{0, \ldots, k-1\}$ denote the $i$-th updated index. Under the bounded-range update model, an updated value is equally likely to fall anywhere within the global value range. Consequently, the probability that the element at position $i$ must move left (L) is proportional to the number of updated indices preceding it, while the probability that it must move right (R) is proportional to the number following it. We model this as
\[
\Pr[\text{L at position } i] = \frac{i}{k-1},
\qquad
\Pr[\text{R at position } i] = 1 - \frac{i}{k-1} = \frac{k-1-i}{k-1}.
\]

By Definition~\ref{def:segment}, segment boundaries occur exactly at transitions of the form $\mathrm{L}\!\to\!\mathrm{R}$ in the direction sequence of updated indices. Therefore, the number of segments equals one plus the number of $\mathrm{LR}$ transitions.

\paragraph{Expected number of $\mathrm{LR}$ transitions.}
For adjacent positions $(i, i+1)$ with $i \in \{0, \ldots, k-2\}$, the probability
of an $\mathrm{LR}$ transition is
\[
\Pr[\mathrm{LR} \text{ at } (i, i+1)]
=
\Pr[\mathrm{L \ at\ } i] \cdot \Pr[\mathrm{R \ at\ } i+1]
=
\frac{i}{k-1} \cdot \frac{k-2-i}{k-1}
=
\frac{i(k-2-i)}{(k-1)^2}.
\]

Summing over all adjacent pairs yields
\[
\mathbb{E}[\mathrm{LR\ transitions}]
=
\sum_{i=0}^{k-2} \frac{i(k-2-i)}{(k-1)^2}
=
\frac{1}{(k-1)^2} \sum_{i=0}^{k-2} i(k-2-i).
\]

Using standard summation identities,
\[
\sum_{i=0}^{k-2} i(k-2-i)
=
\frac{(k-2)(k-1)k}{6},
\]
and therefore
\[
\mathbb{E}[\mathrm{LR\ transitions}]
=
\frac{(k-2)k}{6(k-1)}
=
\Theta(k).
\]

\paragraph{Expected number of segments.}
The expected number of segments is one plus the expected number of
$\mathrm{LR}$ transitions, which is $\Theta(k)$.

\paragraph{Movement bound.}
The $\Theta(k)$ segments partition the array into disjoint regions. Under a
uniform distribution of updated indices, each segment spans $O(n/k)$ array
positions in expectation. By Lemma~\ref{lem:confinement}, all data movement
during Phase~2 is confined within segment boundaries.

Each updated element moves at most the width of its containing segment.
Hence,
\[
\mathbb{E}[\text{total movement}]
\le
k \cdot O\!\left(\frac{n}{k}\right)
=
O(n).
\]

Thus, the expected total data movement during the repair phase is linear in
the array size and independent of the number of updates.
\end{proof}

\subsection{Behavior Analysis}

We discuss 2 key behavioral aspects of DeltaSort:

\begin{enumerate}
  \item \textbf{Stability}: DeltaSort's stability depends on the underlying sort used in Phase~1. Using a stable sort ensures that equal updated values maintain their relative order, preserving overall stability.
  \item \textbf{Sensitivity to Updates}: The performance of DeltaSort is sensitive to the number and distribution of updates. As the number of updates $k$ increases, the algorithm adapts by creating more segments, balancing movement and comparison costs.
\end{enumerate}

%==============================================================================
\section{Experimental Evaluation}
\label{sec:experiments}
%==============================================================================

\subsection{Setup}
All experiments are run on a Rust implementation of DeltaSort~\cite{deltasort-repo} on a synthetic dataset of user objects with composite keys (country, age, name) on an M3 Pro MacBook Pro with 18GB RAM. Each data point has a CI of $<5\%$. All measurements are taken after warm-up runs to account for caching effects. We deliberately evaluate DeltaSort on fully random update distributions, which provide no favorable structure to the workload (such as bounded-window or smooth-value updates) which can further amplify DeltaSort's advantages. We use a multi-key comparator that lexicographically compares country (string), age (integer), and name (string) to ensure that we use a realistic comparison scenario. All the results are reproducible using the code and data available in the repository~\cite{deltasort-repo}.
DeltaSort is evaluated against the following baselines:

\begin{itemize}
    \item \textbf{NativeSort}: Rust’s \texttt{sort\_by} implementation (PDQSort~\cite{peters2021pdqsort}), representing the standard approach of re-sorting the entire array. NativeSort is \emph{not} update-aware, but serves as an important baseline to quantify the benefit of exposing update information to the sorting routine. We expect update-aware algorithms to outperform NativeSort only up to a critical batch size $k_c$, beyond which full re-sorting becomes preferable. Benchmarking NativeSort is therefore essential to identify this crossover point.

    \item \textbf{Binary-Insertion-Sort (BIS)}: Extract each updated value and reinsert it independently using binary search. This approach performs $O(k \log n)$ comparisons but may incur $O(kn)$ data movement in the worst case. BIS is highly comparison-efficient and is a common choice for very small update batches.

    \item \textbf{Extract-Sort-Merge (ESM)}: Extract all updated values, sort them separately, and merge them with the remaining sorted portion of the array. This approach performs $O(k \log k + n)$ comparisons and $O(n)$ data movement. ESM is more movement-efficient than BIS for larger $k$, but incurs higher comparison cost.
\end{itemize}

Together, these baselines provide strong baselines for comparison across the full range of update batch sizes ($k$).
\input{figures/sorting-algorithms}

\subsection{Correctness}
Correctness is formally proven in Theorem~\ref{thm:correctness} and also verified by an extensive set of randomized tests~\cite{deltasort-repo} across various scales and update sizes. The test routine generates a sorted base array of size $n$, applies $k$ random updates at random indices, runs DeltaSort, and asserts that the final array is sorted and contains all original values with updated values.

\subsection{Execution Time}

Figure~\ref{fig:rust-execution-time} shows execution time (in microseconds) for $n = 50$K values as a function of updated count $k$. DeltaSort consistently outperforms all alternatives up to approximately $k = 16.5$K (crossover point), achieving significant speedups of $4$--$20\times$ over NativeSort in the intermediate range. Also note the orders-of-magnitude gap between DeltaSort and the baseline incremental algorithms (BIS and ESM) across the full range of $k$ values.

\input{figures/rust/execution-time}

There are many interesting observations from the execution time graph:
\begin{enumerate}
  \item Even though NativeSort (PDQSort~\cite{peters2021pdqsort}) is slower than update-aware algorithms (which is expected), it is able to effectively leverage partial order for small $k$ values, resulting in sub-linear time complexity in that range. After $k \gtrapprox 0.2\%$, NativeSort takes roughly the same amount of time irrespective of $k$.
  \item BIS performs well for $k \lessapprox 0.004\% $ (still not the fastest) but quickly degrades into the worst performing algorithm due to high data movement cost ($O(kn)$).
  \item ESM is the best performing baseline for most of the range $(1\% \gtrapprox k \lessapprox 80\%)$. So if updates are known and in this range, ESM is the fastest option at the cost of $O(n)$ auxiliary space. 
  \item DeltaSort outperforms both BIS and ESM by $\approx$ $2$--$5$$\times$ for $k \lessapprox 1\%$. It uses $O(k)$ auxiliary space which is more than BIS ($O(1)$) but less than ESM ($(O(n)$)). Also, even though the range is small in absolute terms, in real-world scenarios the typical udpate sizes fall in this range.
\end{enumerate}

The above observations indicate that just like we have hybrid blind sorting algorithms (e.g., TimSort~\cite{timsort}, PDQSort~\cite{peters2021pdqsort}) that adapt to input structure, we can also have hybrid \emph{update-aware} sorting algorithms that adapt to update size depending on the kind of execution environment we are in (memory-constrained or compute-constrained). We will explore this direction further in future work.

\subsection{Comparator Invocation Count}

Figure~\ref{fig:rust-comparator-count} shows the number of comparator invocations for each algorithm. DeltaSort and BIS both achieve $O(k \log n)$ comparisons, substantially fewer than NativeSort's $O(n \log n)$ and ESM's $O(k \log k + n)$. The comparison counts for DeltaSort are 10--40\% higher than BIS, indicating that DeltaSort's performance advantage comes from reduced data movement (Lemma~\ref{lem:confinement}). This also suggests that in scenarios with expensive comparators, the speedups from \emph{DeltaSort} over NativeSort may be even greater.

\input{figures/rust/comparator-count}

We can observe following patterns form the comparator count graph:
\begin{enumerate}
  \item NativeSort's comparator count remains roughly constant after $k \gtrapprox 0.2\%$, mirroring its execution time behavior.
  \item BIS is the most comparison-efficient algorithm.
  \item ESM's comparator count is consistently high even for small $k$ and approaches that of NativeSort for large $k$.
  \item DeltaSort's comparator count is similar to BIS for $k \lessapprox 0.1\%$ and is $\approx 10-20\%$ higher for the remaining range.
\end{enumerate}

The key takeway is that as comparator cost grows, DeltaSort's relative advantage over ESM and NativeSort increases while still remaining faster than BIS for $k \lessapprox 1\%$. We chose a typical comparator for our experiments. In future work, we will explore the performance patterns across a range of comparator costs.

\subsection{Crossover Threshold Analysis}

A key practical question is: at what update count should one switch from an update-aware algorithm to NativeSort? We conducted binary search for the crossover point $k_c$ across array sizes from 1K to 10M values for all update-aware algorithms.

Figure~\ref{fig:rust-crossover-all} shows the crossover thresholds for all three update-aware algorithms versus NativeSort. BIS has the lowest crossover threshold due to its $O(kn)$ time complexity, making it practical only for very small update counts. ESM achieves the highest crossover threshold thanks to its $O(n + k \log k)$ time complexity, but requires $O(n)$ auxiliary space. DeltaSort occupies the middle ground---higher crossover than BIS while using only $O(k)$ space instead of ESM's $O(n)$.

\input{figures/rust/crossover-all}

Beyond comparing each algorithm to NativeSort, we also measured the crossover threshold for DeltaSort versus ESM directly. This comparison is particularly relevant because ESM is the strongest baseline in terms of asymptotic time complexity.

\input{figures/rust/crossover-ds-vs-esm}

Figure~\ref{fig:rust-crossover-ds-vs-esm} shows that DeltaSort outperforms ESM for update fractions below the crossover curve while using only $O(k)$ space compared to ESM's $O(n)$. This demonstrates that DeltaSort provides both time and space benefits for moderate update sizes---the common case in incremental maintenance scenarios.

The key takeaway is that DeltaSort offers the best incremental sort performance for \textbf{large} ranges of update sizes (0--30\% for the dataset we tested). The exact crossover threshold depends on the specific scenario (array size, data types, comparator cost, etc.).

\subsection{Performance in managed execution environments}

DeltaSort was also implemented in JavaScript~\cite{deltasort-repo} and benchmarked on the V8 engine to evaluate behavior in managed runtimes. Initial results indicate that DeltaSort outperforms NativeSort for some workloads, but the gains are more modest and the crossover point occurs at smaller update sizes. This behavior appears to be driven primarily by the performance characteristics of the native JavaScript sort, which is highly optimized and handles nearly sorted inputs exceptionally well. As a result, the relative advantage of segmented repair is reduced in this environment compared to the Rust implementation. The JavaScript benchmarks are still being refined and will be reported in a later revision. Until then, the Rust implementation provides the primary and authoritative performance characterization.

%==============================================================================
\section{Future Work}
\label{sec:future}
%==============================================================================

This work opens up several directions for future investigation:

\begin{itemize}
  \item \textbf{Explore parallelizable segmentation:}
  The independent segments created in Phase~1 suggest opportunities for parallel repair. Investigating parallel implementations of DeltaSort could yield further performance improvements on multi-core systems or distributed environments.

  \item \textbf{Analyze structured workloads:}
  The current evaluation tests randomized updates, whereas many real workloads exhibit additional structure, such as gradual value changes in leaderboards or localized updates in interactive list views. Studying such patterns may reveal regimes where DeltaSort’s advantages are amplified or diminished.

  \item \textbf{Study managed environments:}
  Performance variance in managed environments warrants deeper investigation. Understanding the impact of factors such as garbage collection and JIT compilation will help explain the observed performance characteristics.

  \item \textbf{Analyze block-structured storage:}
  Although this work focuses on in-memory arrays, the update-aware model naturally extends to block-structured storage. Exploring how DeltaSort-style segmentation interacts with page- or block-based layouts may clarify its applicability to database and external-memory settings.
\end{itemize}

%==============================================================================
\section{Conclusion}
\label{sec:conclusion}
%==============================================================================

This paper introduced \emph{DeltaSort}, an incremental repair algorithm for maintaining sorted arrays. The key insight is that pre-sorting updated values induces segmentation: updated values naturally partition into segments that can be repaired independently. DeltaSort leverages this segmentation through stack-based processing. L-moving values are repaired immediately with progressively narrowed search ranges, while R-moving values are deferred and processed in reverse order to ensure stable target positions. This segmentation avoids redundant comparisons and overlapping value movement that arise from repeated binary insertion. An experimental evaluation in Rust demonstrates that DeltaSort outperforms both blind native sorting and repeated binary insertion across a wide range of array sizes and update volumes.

More broadly, this work highlights the value of integrating application-level update information into core algorithms. When sorting routines are informed of which values changed, batching and segmentation become possible, enabling performance improvements that blind algorithms cannot realize. DeltaSort illustrates how modest structural insight---segmentation combined with disciplined processing order---can yield substantial practical gains.

%==============================================================================
\bibliographystyle{plain}
\bibliography{refs}
\end{document}
